{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Workshop2: 3. Conditional GAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "AiVv7eTcUrXs"
      },
      "cell_type": "markdown",
      "source": [
        "# Conditional GAN\n",
        "\n",
        "![alt text](https://i.imgur.com/jgtlRHS.png)"
      ]
    },
    {
      "metadata": {
        "id": "RZgxssg12CSJ"
      },
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision matplotlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TiZEqv-zIkhF"
      },
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3R0H-oMEIkl_"
      },
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oOrOWjdJE3Fl"
      },
      "cell_type": "markdown",
      "source": [
        "In thic cell we initialize the data loaders for the MNIST dataset that will be used for provide data to training loop later.\n",
        "\n",
        "Here, we also specify size of the minibatch.\n",
        "\n",
        "NOTE: To keep our network simple and training time short, we take only samples of classes 0, 1 and 2"
      ]
    },
    {
      "metadata": {
        "id": "MPBY-H-zIku0"
      },
      "cell_type": "code",
      "source": [
        "batch_size = 50\n",
        "\n",
        "train_data = datasets.MNIST('data/mnist', train=True, download=True,\n",
        "                           transform=transforms.ToTensor())\n",
        "\n",
        "classes = [0, 1, 2]\n",
        "filter_labels = [i for i, l in enumerate(train_data.train_labels) if l in classes]\n",
        "\n",
        "train_data.train_data = train_data.train_data[filter_labels]\n",
        "train_data.train_labels = train_data.train_labels[filter_labels]\n",
        "\n",
        "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, \n",
        "                              num_workers=4, pin_memory=True, drop_last=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wg-kW8y2FGYi"
      },
      "cell_type": "markdown",
      "source": [
        "In this cell we specify dimensions of vectors for:\n",
        "* `X_len` - linearized images (MNIST containes images of size $28 \\times 28 = 784$)\n",
        "* `z_len` - encoding vector. Here, 64 is used, but you can try smaller or larger vectors\n",
        "* `c_len` - code vactor, the same length as the number of classes that are in our dataset"
      ]
    },
    {
      "metadata": {
        "id": "Hn59s8CKIkxn"
      },
      "cell_type": "code",
      "source": [
        "X_len = 28 * 28\n",
        "z_len = 64\n",
        "c_len = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pM8KwaXUFsYY"
      },
      "cell_type": "markdown",
      "source": [
        "Model of a Generator $G: (z, c) \\rightarrow X$ - takes feature vecor $z$ sampled from unit gaussian distribution $z \\sim \\mathcal{N}(0, I)  \\in \\mathbb{R}^{h}$ and code one-hot vector $c \\in \\{0, 1\\}^{\\text{c_len}}$ (one-hot - has value `1` on exactly one position). Produces a linearized image $X \\in [0, 1]^{784}$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "RTwajJ8fIk2n"
      },
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self, X_dim, c_dim, z_dim):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.model = nn.Sequential(\n",
        "      torch.nn.Linear(z_dim + c_dim, 128),\n",
        "      torch.nn.ReLU(),\n",
        "      torch.nn.Linear(128, X_dim),\n",
        "      torch.nn.Sigmoid()\n",
        "    )\n",
        "  \n",
        "  def forward(self, z, c):\n",
        "    zc = torch.cat([z, c], dim=1)\n",
        "    X = self.model(zc)\n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gh9hKrmLG2ds"
      },
      "cell_type": "markdown",
      "source": [
        "Model of a Discriminator $D: (X, c) \\rightarrow [0, 1]$ - module that (given the code vector) should give high probability $p$ for samples from training dataset and low probability $p$ for generated samples.\n"
      ]
    },
    {
      "metadata": {
        "id": "hy2aP_BVIk4n"
      },
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, X_dim, c_dim):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.model = nn.Sequential(\n",
        "      torch.nn.Linear(X_dim + c_dim, 128),\n",
        "      torch.nn.ReLU(),\n",
        "      torch.nn.Linear(128, 1),\n",
        "      torch.nn.Sigmoid()\n",
        "    )\n",
        "  \n",
        "  def forward(self, X, c):\n",
        "    Xc = torch.cat([X, c], dim=1)\n",
        "    p = self.model(Xc)\n",
        "    return p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e3BtkL0tIk7P"
      },
      "cell_type": "code",
      "source": [
        "G = Generator(X_dim=X_len, c_dim=c_len, z_dim=z_len).to(device)\n",
        "D = Discriminator(X_dim=X_len, c_dim=c_len).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v_JfxIMFBocu"
      },
      "cell_type": "markdown",
      "source": [
        "Weight initialization - we use the weight initialization from \"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\" by He et al."
      ]
    },
    {
      "metadata": {
        "id": "XyTWr8NsIk9u"
      },
      "cell_type": "code",
      "source": [
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname in ('Conv1d', 'Linear'):\n",
        "        torch.nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "        if m.bias is not None:\n",
        "            torch.nn.init.constant_(m.bias, 0)\n",
        "\n",
        "G = G.apply(weights_init)\n",
        "D = D.apply(weights_init)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tCVxSOHWT9L8"
      },
      "cell_type": "markdown",
      "source": [
        "Define optimizers that will calculate optimization steps for our weights. Note, that Encoder and Generator share the same optimizer. Here we use Adam from \"Adam: A Method for Stochastic Optimization\" by Kingma et al."
      ]
    },
    {
      "metadata": {
        "id": "6NwLPdhqIlBV"
      },
      "cell_type": "code",
      "source": [
        "learning_rate = 3e-4\n",
        "betas = (0.9, 0.999)\n",
        "\n",
        "G_optimizer = optim.Adam(G.parameters(), lr=learning_rate, betas=betas)\n",
        "D_optimizer = optim.Adam(D.parameters(), lr=learning_rate, betas=betas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6ZD27ibUdLWF"
      },
      "cell_type": "markdown",
      "source": [
        "Labels for Discriminator: \n",
        "\n",
        "1 - true sample\n",
        "\n",
        "0 - generated samples\n",
        "\n",
        "Labels for Generator:\n",
        "\n",
        "1 - generated samples"
      ]
    },
    {
      "metadata": {
        "id": "MDFu0cc7S25_"
      },
      "cell_type": "code",
      "source": [
        "ones = torch.ones(batch_size, 1).to(device)\n",
        "zeros = torch.zeros(batch_size, 1).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0cGWlRSydtGO"
      },
      "cell_type": "markdown",
      "source": [
        "Losses:\n",
        "\n",
        "$L_D = \\text{bce}(D(X, c), 1) + \\text{bce}(D(G(z, c), c), 0)$\n",
        "\n",
        "$L_G = \\text{bce}(D(G(z, c), c), 1)$\n",
        "\n",
        "where `bce` - binary cross-entropy, defined in `torch.nn` module, `1` - vector of ones, `0` - vector of zeros."
      ]
    },
    {
      "metadata": {
        "id": "J2FyCnoKIkkY"
      },
      "cell_type": "code",
      "source": [
        "def loss_fn_g(p_gen):\n",
        "  return None\n",
        "\n",
        "def loss_fn_d(p_real, p_gen):\n",
        "  return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xmPISJhxPFUq"
      },
      "cell_type": "markdown",
      "source": [
        "Training procedure for Conditional GAN - fill the training steps, that are currently `None`"
      ]
    },
    {
      "metadata": {
        "id": "E6LQ0QzBI4w0"
      },
      "cell_type": "code",
      "source": [
        "max_epochs = 300\n",
        "for epoch_n in range(1, max_epochs+1):\n",
        "  \n",
        "  D.train()\n",
        "  G.train()\n",
        "\n",
        "  d_losses = 0.0\n",
        "  g_losses = 0.0\n",
        "  \n",
        "  start = datetime.now()\n",
        "  for i, (X, c) in enumerate(train_dataloader, 1):\n",
        "    X = X.to(device)\n",
        "    c = c.to(device)\n",
        "    \n",
        "    X = X.view(X.size(0), -1)\n",
        "    \n",
        "    # convert c to one-hot representation\n",
        "    c_one_hot = None\n",
        "   \n",
        "    z = None\n",
        "    X_ = None\n",
        "  \n",
        "    p_real = None\n",
        "    p_gen = None\n",
        "    \n",
        "    D_optimizer.zero_grad()\n",
        "    D.zero_grad()\n",
        "    loss_d  = loss_fn_d(p_real, p_gen)\n",
        "    loss_d.backward(retain_graph=True)\n",
        "    d_losses += loss_d.item()\n",
        "    D_optimizer.step()\n",
        "    \n",
        "    \n",
        "    G_optimizer.zero_grad()\n",
        "    G.zero_grad()\n",
        "    loss_g = loss_fn_g(p_gen)\n",
        "    loss_g.backward()\n",
        "    g_losses += loss_g.item()\n",
        "    G_optimizer.step()\n",
        "   \n",
        "  print(f'Epoch {epoch_n:03d}: Loss_G: {g_losses / i:.4f} Loss_D: {d_losses / i:.4f}  Time: {datetime.now() - start}')\n",
        "  \n",
        "  \n",
        "  # Visualize learing\n",
        "  n_samples = 10\n",
        "  with torch.no_grad():\n",
        "    fig, ax = plt.subplots(c_len, n_samples, figsize=(10, c_len))\n",
        "    fig.suptitle(f'Conditional samples: {epoch_n}')\n",
        "    for c in range(c_len):\n",
        "      c_one_hot = torch.zeros((n_samples, c_len)).to(device)\n",
        "      c_one_hot[torch.arange(n_samples), c] = 1\n",
        "      \n",
        "      z = torch.randn(n_samples, z_len).to(device) \n",
        "\n",
        "      samples = G(z, c_one_hot).view(n_samples, 28, 28).cpu().numpy()\n",
        "      for i, sample in enumerate(samples):\n",
        "        ax[c][i].imshow(sample)\n",
        "        ax[c][i].axis('off')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "om3HMMB0RbDA"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}