{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Workshop2: 4. InfoGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pGeI31oUyLO"
      },
      "source": [
        "# InfoGAN\n",
        "\n",
        "![alt text](https://i.imgur.com/zAopCOi.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpGPZVdC18aw"
      },
      "source": [
        "!pip install matplotlib numpy torch torchvision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjwlAlF_ZmFR"
      },
      "source": [
        "from datetime import datetime\n",
        "from itertools import chain\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZ69Ih0QZmIS"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6u9JI2YQFGv"
      },
      "source": [
        "In thic cell we initialize the data loaders for the MNIST dataset that will be used for provide data to training loop later.\n",
        "\n",
        "Here, we also specify size of the minibatch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzwdH57GZmKb"
      },
      "source": [
        "batch_size = 50\n",
        "\n",
        "train_data = datasets.MNIST('data/mnist', train=True, download=True,\n",
        "                           transform=transforms.ToTensor())\n",
        "\n",
        "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, \n",
        "                              num_workers=4, pin_memory=True, drop_last=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIUxIcP8Qg5r"
      },
      "source": [
        "In this cell we specify dimensions of vectors for:\n",
        "* `X_len` - linearized images (MNIST containes images of size $28 \\times 28 = 784$)\n",
        "* `z_len` - encoding vector. Here, 16 is used, but you can try smaller or larger (especially if images are of poor quality) vectors\n",
        "* `c_len` - code vactor,  that we decided to be a categorical variable represented as one-hot vector of length 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zDEqm6YZmOi"
      },
      "source": [
        "X_len = 28 * 28\n",
        "z_len = 16\n",
        "c_len = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0WtABS3aobr"
      },
      "source": [
        "Generator\n",
        "\n",
        "Model of a Generator $G: (z, c) \\rightarrow X$ - takes feature vecor $z$ sampled from unit gaussian distribution $z \\sim \\mathcal{N}(0, I)  \\in \\mathbb{R}^{h}$ and code one-hot vector $c \\in \\{0, 1\\}^{\\text{c_len}}$ (one-hot - has value `1` on exactly one position). Produces a linearized image $X \\in [0, 1]^{784}$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvJeUG0_ZmRP"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self, X_dim, z_dim, c_dim):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.model = nn.Sequential(\n",
        "      torch.nn.Linear(z_dim + c_dim, 128),\n",
        "      torch.nn.ReLU(),\n",
        "      torch.nn.Linear(128, X_dim),\n",
        "      torch.nn.Sigmoid()\n",
        "    )\n",
        "  \n",
        "  def forward(self, z, c):\n",
        "    zc = torch.cat([z, c], dim=1)\n",
        "    X = self.model(zc)\n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLm2tt7LbJKa"
      },
      "source": [
        "Model of a Discriminator $D: X \\rightarrow [0, 1]$ - module that should give high probability $p$ for samples from training dataset and low probability $p$ for generated samples.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJUG3acVZmTn"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, X_dim):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.model = nn.Sequential(\n",
        "      torch.nn.Linear(X_dim, 128),\n",
        "      torch.nn.ReLU(),\n",
        "      torch.nn.Linear(128, 1),\n",
        "      nn.Sigmoid()\n",
        "    )\n",
        "    \n",
        "  \n",
        "  def forward(self, X):\n",
        "    p = self.model(X)\n",
        "    return p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tw-Vfd6zbNXd"
      },
      "source": [
        "Model of a mutual information enforcer $Q : X \\rightarrow c $. Given the image $X \\in [0, 1]^{784 \\times 1} $ perform an embedding into one-hot vector $c \\in [0, 1]^{|c|} $, that will maximize mutual information between representations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAaCu5nKd-DV"
      },
      "source": [
        "class Q(nn.Module):\n",
        "  def __init__(self, X_dim, c_dim):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.model = nn.Sequential(\n",
        "      torch.nn.Linear(X_dim, 128),\n",
        "      torch.nn.ReLU(),\n",
        "      torch.nn.Linear(128, c_dim),\n",
        "      torch.nn.Softmax()\n",
        "    )\n",
        "  \n",
        "  def forward(self, X):\n",
        "    c = self.model(X)\n",
        "    return c"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmjH9uGNZmWB"
      },
      "source": [
        "G = Generator(X_dim=X_len, z_dim=z_len, c_dim=c_len).to(device)\n",
        "D = Discriminator(X_dim=X_len).to(device)\n",
        "Q_ = Q(X_dim=X_len, c_dim=c_len).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHMs-DRHBrRj"
      },
      "source": [
        "Weight initialization - we use the weight initialization from \"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\" by He et al."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtQ-hTwcZmYi"
      },
      "source": [
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname in ('Conv1d', 'Linear'):\n",
        "        torch.nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "        if m.bias is not None:\n",
        "            torch.nn.init.constant_(m.bias, 0)\n",
        "\n",
        "G = G.apply(weights_init)\n",
        "D = D.apply(weights_init)\n",
        "Q_ = Q_.apply(weights_init)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFQPdb5QT-TN"
      },
      "source": [
        "Define optimizers that will calculate optimization steps for our weights. Note, that Encoder and Generator share the same optimizer. Here we use Adam from \"Adam: A Method for Stochastic Optimization\" by Kingma et al."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZCT-A-sZmbW"
      },
      "source": [
        "learning_rate = 3e-4\n",
        "betas = (0.9, 0.999)\n",
        "\n",
        "G_optimizer = optim.Adam(G.parameters(), lr=learning_rate, betas=betas)\n",
        "D_optimizer = optim.Adam(D.parameters(), lr=learning_rate, betas=betas)\n",
        "Q_optimizer = optim.Adam(chain(Q_.parameters(), G.parameters()), lr=learning_rate, betas=betas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FvvYZRyX8Q_"
      },
      "source": [
        "Losses:\n",
        "\n",
        "$L_Q = H(X) - H(X|Y) \\simeq -H(X|Y) = - \\sum c*\\log Q_{c|X}$\n",
        "\n",
        "$L_G = \\log(D(G(z, c))$\n",
        "\n",
        "$L_D = \\log(D(X)) + \\log(1 - D(G(z, c)))$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbpKb7_nZmeB"
      },
      "source": [
        "def loss_fn_d(p_real, p_gen):\n",
        "  eps = 1e-8\n",
        "  return None\n",
        "\n",
        "def loss_fn_g(p_gen):\n",
        "  eps = 1e-8\n",
        "  return None\n",
        "  \n",
        "def loss_fn_q(c, q_c_given_x):\n",
        "  eps = 1e-8\n",
        "  return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfGTtZbqZmjO"
      },
      "source": [
        "max_epochs = 300\n",
        "for epoch_n in range(1, max_epochs+1):\n",
        "  \n",
        "  D.train()\n",
        "  G.train()\n",
        "  Q_.train()\n",
        "\n",
        "  d_losses = 0.0\n",
        "  g_losses = 0.0\n",
        "  q_losses = 0.0\n",
        "  \n",
        "  start = datetime.now()\n",
        "  for i, (X, y) in enumerate(train_dataloader, 1):\n",
        "    X = X.to(device)\n",
        "    y = y.to(device)\n",
        "    \n",
        "    X = X.view(X.size(0), -1)\n",
        "   \n",
        "    z = None\n",
        "    c = None\n",
        "    c = None\n",
        "    \n",
        "    # Train discriminator \n",
        "    D_optimizer.zero_grad()\n",
        "    \n",
        "    X_ = None\n",
        "  \n",
        "    p_real = None\n",
        "    p_gen = None\n",
        "    \n",
        "    loss_d  = -torch.mean(loss_fn_d(p_real, p_gen))\n",
        "\n",
        "    loss_d.backward(retain_graph=True)\n",
        "    d_losses += loss_d.item()\n",
        "    D_optimizer.step()\n",
        "    \n",
        "    # Train generator\n",
        "    G_optimizer.zero_grad()\n",
        "    \n",
        "    X_ = None\n",
        "    p_gen = None\n",
        "    \n",
        "    loss_g = -torch.mean(loss_fn_g(p_gen))\n",
        "    loss_g.backward(retain_graph=True)\n",
        "    g_losses += loss_g.item()\n",
        "    G_optimizer.step()\n",
        "    \n",
        "    # Train mutual information regularization\n",
        "    Q_optimizer.zero_grad()\n",
        "  \n",
        "    \n",
        "    X_ = None\n",
        "    q_c_given_x = None\n",
        "    \n",
        "    loss_q = torch.mean(loss_fn_q(c, q_c_given_x))\n",
        "    \n",
        "    loss_q.backward()\n",
        "    q_losses += loss_q.item()\n",
        "    Q_optimizer.step()\n",
        "    \n",
        "   \n",
        "  print(f'Epoch {epoch_n:03d}: Loss_G: {g_losses / i:.4f} Loss_Q: {q_losses / i:.4f} Loss_D: {d_losses / i:.4f}  Time: {datetime.now() - start}')\n",
        "  \n",
        "  n_samples = 3\n",
        "  with torch.no_grad():\n",
        "    fig, ax = plt.subplots(n_samples, c_len, figsize=(c_len, n_samples))\n",
        "    fig.suptitle(f'Feature - epoch: {epoch_n}')\n",
        "    for c_ in np.arange(c_len):\n",
        "      for i in np.arange(n_samples): \n",
        "        z = torch.randn(1, z_len).to(device)\n",
        "        c = torch.zeros(1, c_len).to(device)\n",
        "        c[0, c_] = 1.0\n",
        "        sample = G(z, c).to(device).view(28, 28).cpu().numpy()\n",
        "        ax[i][c_].imshow(sample)\n",
        "        ax[i][c_].axis('off')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}