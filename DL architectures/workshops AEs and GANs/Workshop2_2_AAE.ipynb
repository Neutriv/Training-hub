{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Workshop2: 2. AAE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "B6EW3QmWQURU"
      },
      "cell_type": "markdown",
      "source": [
        "# Adversarial AutoEncoder\n",
        "![Adversarial AutoEncoder architecture](https://i.imgur.com/sgsfLwQ.png)"
      ]
    },
    {
      "metadata": {
        "id": "69ktdN1oGPKK"
      },
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "!pip install matplotlib numpy torch torchvision "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JJ4BCi8SF42P"
      },
      "cell_type": "code",
      "source": [
        "# Import necessary modules\n",
        "from datetime import datetime\n",
        "from itertools import chain\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9tTDTLoqGI2-"
      },
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9YW7iUcXWKB5"
      },
      "cell_type": "markdown",
      "source": [
        "In thic cell we initialize the data loaders for the MNIST dataset that will be used for provide data to training loop later.\n",
        "\n",
        "Here, we also specify size of the minibatch."
      ]
    },
    {
      "metadata": {
        "id": "fcfudMWeGJAf"
      },
      "cell_type": "code",
      "source": [
        "batch_size = 100\n",
        "\n",
        "train_data = datasets.MNIST('data/mnist', train=True, download=True,\n",
        "                           transform=transforms.ToTensor())\n",
        "\n",
        "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, \n",
        "                              num_workers=4, pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a274dvXmPfRk"
      },
      "cell_type": "markdown",
      "source": [
        "In this cell we specify dimensions of vectors for:\n",
        "* `X_len` - linearized images (MNIST containes images of size $28 \\times 28 = 784$)\n",
        "* `z_len` - encoding vector. Here, 64 is used, but you can try smaller or larger vectors\n"
      ]
    },
    {
      "metadata": {
        "id": "uTAeQ8Vd86he"
      },
      "cell_type": "code",
      "source": [
        "X_len = 28 * 28\n",
        "z_len = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IYkGpI3AHG2n"
      },
      "cell_type": "markdown",
      "source": [
        "Model of an Encoder $E: X \\rightarrow z$, that takes linearized images $X \\in [0, 1]^{784}$ and produces encoding $z \\in \\mathbb{R}^{h}$ of length $h$ (specified by `z_len` from previous cell).\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "iQlQeVEjGJDG"
      },
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, X_dim, z_dim):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.model = nn.Sequential(\n",
        "      torch.nn.Linear(X_dim, 128),\n",
        "      torch.nn.ReLU(),\n",
        "      torch.nn.Linear(128, z_dim)\n",
        "    )\n",
        "  \n",
        "  def forward(self, X):\n",
        "    z = self.model(X)\n",
        "    return z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ahUHS5j7IPUz"
      },
      "cell_type": "markdown",
      "source": [
        "Model of a Generator $G: z \\rightarrow X$, that takes encoding $z \\in \\mathbb{R}^{h}$ of length $h$ (specified by `z_len` from 2 cells ago) and produces linearized images $X \\in [0, 1]^{784}$.\n"
      ]
    },
    {
      "metadata": {
        "id": "6fOFDwQpGJFu"
      },
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self, X_dim, z_dim):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.model = nn.Sequential(\n",
        "      torch.nn.Linear(z_dim, 128),\n",
        "      torch.nn.ReLU(),\n",
        "      torch.nn.Linear(128, X_dim),\n",
        "      torch.nn.Sigmoid()\n",
        "    )\n",
        "  \n",
        "  def forward(self, z):\n",
        "    X = self.model(z)\n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K207Y03kIzX9"
      },
      "cell_type": "markdown",
      "source": [
        "Model of a Discriminator $D: z \\rightarrow [0, 1]$ - module that should give high probability $p$ for samples that come from the desired unit gaussian distribution $z \\sim \\mathcal{N}(0, I)$ and low probability $p$ for samples $\\hat{z} \\in \\mathbb{R}^{h}$ that come from encoder $E$.\n"
      ]
    },
    {
      "metadata": {
        "id": "F2jN54I9GJH-"
      },
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, z_dim):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.model = nn.Sequential(\n",
        "      torch.nn.Linear(z_dim, 128),\n",
        "      torch.nn.ReLU(),\n",
        "      torch.nn.Linear(128, 1),\n",
        "      torch.nn.Sigmoid()\n",
        "    )\n",
        "  \n",
        "  def forward(self, z):\n",
        "    p = self.model(z)\n",
        "    return p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZLfe_8Pi8mmZ"
      },
      "cell_type": "code",
      "source": [
        "E = Encoder(X_dim=X_len, z_dim=z_len).to(device)\n",
        "G = Generator(X_dim=X_len, z_dim=z_len).to(device)\n",
        "D = Discriminator(z_dim=z_len).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vheUa9W8Bmog"
      },
      "cell_type": "markdown",
      "source": [
        "Weight initialization - we use the weight initialization from \"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\" by He et al."
      ]
    },
    {
      "metadata": {
        "id": "HyPKkeQLvdtP"
      },
      "cell_type": "code",
      "source": [
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname in ('Conv1d', 'Linear'):\n",
        "        torch.nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "        if m.bias is not None:\n",
        "            torch.nn.init.constant_(m.bias, 0)\n",
        "\n",
        "E = E.apply(weights_init)\n",
        "G = G.apply(weights_init)\n",
        "D = D.apply(weights_init)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dtaCySwJBwju"
      },
      "cell_type": "markdown",
      "source": [
        "Define optimizers that will calculate optimization steps for our weights. Note, that Encoder and Generator share the same optimizer. Here we use Adam from \"Adam: A Method for Stochastic Optimization\" by Kingma et al."
      ]
    },
    {
      "metadata": {
        "id": "gjqpgx_59fw0"
      },
      "cell_type": "code",
      "source": [
        "learning_rate = 3e-4\n",
        "betas = (0.9, 0.999)\n",
        "\n",
        "EG_optimizer = optim.Adam(chain(E.parameters(), G.parameters()), \n",
        "                          lr=learning_rate, betas=betas)\n",
        "D_optimizer = optim.Adam(D.parameters(), lr=learning_rate, betas=betas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rP6XZyuOB_6k"
      },
      "cell_type": "markdown",
      "source": [
        "Define the losses for AAE here. Remember to add epsilon ($10^{-8}$ is enough), where might be numerical instability (e.g. low values passed to logarithm function).\n",
        "\n",
        "Losses for AAE are:\n",
        "\n",
        "$L_D = \\log(D(z)) + \\log(1 - D(E(X)))$\n",
        "\n",
        "$L_{EG} = \\log(D(E(X)))$\n",
        "\n",
        "$L_{reconstruction} = \\text{bce}(X, G(E(X)))$\n",
        "\n",
        "where $\\log$ means natural logarithm, $z \\sim \\mathcal{N}(0, 1)$ and bce - binary crossentropy loss implemented in `torch.nn` module"
      ]
    },
    {
      "metadata": {
        "id": "prsS0CSJ8m3l"
      },
      "cell_type": "code",
      "source": [
        "def loss_fn_eg(p_gen):\n",
        "  eps = 1e-8\n",
        "  return None\n",
        "  \n",
        "def loss_fn_d(p_real, p_gen):\n",
        "  eps = 1e-8\n",
        "  return None\n",
        "\n",
        "def loss_fn_reconstruction(X, X_):\n",
        "  return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vjq1Mh71EUTn"
      },
      "cell_type": "markdown",
      "source": [
        "Training procedure for AAE - fill the training steps, that are currently `None`"
      ]
    },
    {
      "metadata": {
        "id": "AuFi3BnWGJKD"
      },
      "cell_type": "code",
      "source": [
        "max_epochs = 300\n",
        "for epoch_n in range(1, max_epochs+1):\n",
        "  \n",
        "  D.train()\n",
        "  E.train()\n",
        "  G.train()\n",
        "\n",
        "  reconstruction_losses = 0.0\n",
        "  d_regularization_losses = 0.0\n",
        "  eg_regularization_losses = 0.0\n",
        "  \n",
        "  start = datetime.now()\n",
        "  for i, (X, y) in enumerate(train_dataloader, 1):\n",
        "    X = X.to(device)\n",
        "    y = y.to(device)\n",
        "    \n",
        "    X = X.view(X.size(0), -1)\n",
        "    \n",
        "    z_ = None\n",
        "    X_ = None\n",
        "    \n",
        "    #\n",
        "    # Reconstruction stage\n",
        "    #\n",
        "    loss_rec = loss_fn_reconstruction(X_, X)\n",
        "    \n",
        "    loss_rec.backward(retain_graph=True)\n",
        "    reconstruction_losses += loss_rec.item()\n",
        "    EG_optimizer.step()\n",
        "\n",
        "    EG_optimizer.zero_grad()\n",
        "    E.zero_grad()\n",
        "    G.zero_grad()\n",
        "    \n",
        "    #\n",
        "    # Regularization stage\n",
        "    #\n",
        "    z = None\n",
        "    p_real = None\n",
        "    p_gen = None\n",
        "    \n",
        "    loss_d  = -torch.mean(loss_fn_d(p_real, p_gen))\n",
        "    loss_d.backward(retain_graph=True)\n",
        "    d_regularization_losses += loss_d.item()\n",
        "    D_optimizer.step()\n",
        "    \n",
        "    loss_eg = -torch.mean(loss_fn_eg(p_gen))\n",
        "    loss_eg.backward()\n",
        "    eg_regularization_losses += loss_eg.item()\n",
        "    EG_optimizer.step()\n",
        "    \n",
        "    D_optimizer.zero_grad()\n",
        "    D.zero_grad()\n",
        "    EG_optimizer.zero_grad()\n",
        "    E.zero_grad()\n",
        "    G.zero_grad()\n",
        "  \n",
        "   \n",
        "  print(f'Epoch {epoch_n:03d}: Z mean/std: {z_.mean():.4f}/{z_.std():.4f} '\n",
        "        f'Loss_EG_REC: {reconstruction_losses / i:.4f} Loss_EG_REG: {eg_regularization_losses / i:.4f} '\n",
        "        f'Loss_D_REG: {d_regularization_losses / i:.4f}  Time: {datetime.now() - start}')\n",
        "  \n",
        "  \n",
        "  # Visualize learning\n",
        "  with torch.no_grad():\n",
        "    X = X[:10]\n",
        "    reconstructions = G(E(X)).view(10, 28, 28).cpu().numpy()\n",
        "\n",
        "    reals = X.view(10, 28, 28).cpu().numpy()\n",
        "\n",
        "    z = torch.randn(10, z_len).to(device)\n",
        "    samples = G(z).view(10, 28, 28).cpu().numpy()\n",
        "\n",
        "    fig, ax = plt.subplots(1, 10, figsize=(5, 1))\n",
        "    fig.suptitle(f'Real: {epoch_n}')\n",
        "    for i, real in enumerate(reals):\n",
        "      ax[i].imshow(real)\n",
        "      ax[i].axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    fig, ax = plt.subplots(1, 10, figsize=(5, 1))\n",
        "    fig.suptitle(f'Reconstructions: {epoch_n}')\n",
        "    for i, reconstruction in enumerate(reconstructions):\n",
        "      ax[i].imshow(reconstruction)\n",
        "      ax[i].axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    fig, ax = plt.subplots(1, 10, figsize=(5, 1))\n",
        "    fig.suptitle(f'Synthetic: {epoch_n}')\n",
        "    for i, sample in enumerate(samples):\n",
        "      ax[i].imshow(sample)\n",
        "      ax[i].axis('off')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ry4cl7nERGoQ"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}