{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Workshop1: 2. DC-GAN MNIST - Solved",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "28w-2wozWUAE",
        "outputId": "05a34e40-a389-4556-cfc0-7221108d13b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# check Pillow version\n",
        "import PIL\n",
        "print(PIL.PILLOW_VERSION)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g84c6ZmMTJ16",
        "outputId": "09046393-18b2-4cae-ea8d-396fffb845c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "cell_type": "code",
      "source": [
        "# we need pillow version of 5.3.0\n",
        "# we will uninstall the older version first\n",
        "!pip uninstall -y Pillow\n",
        "\n",
        "# install the new one\n",
        "!pip install Pillow==5.3.0\n",
        "\n",
        "# import the new one\n",
        "import PIL\n",
        "print(PIL.PILLOW_VERSION)\n",
        "# this should print 5.3.0. If it doesn't, then restart your runtime:\n",
        "# Menu > Runtime > Restart Runtime"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling Pillow-5.3.0:\n",
            "  Successfully uninstalled Pillow-5.3.0\n",
            "Collecting Pillow==5.3.0\n",
            "  Using cached https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Installing collected packages: Pillow\n",
            "Successfully installed Pillow-5.3.0\n",
            "5.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tx1uhDBjpPP_",
        "outputId": "f713523b-5eb6-4df0-a303-74de19436278",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "! pip3 install torch torchvision"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (5.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "V9zCMsztz1o3"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "directory = '/content/results/'\n",
        "if not os.path.exists(directory):\n",
        "  os.makedirs(directory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vFHPeLZx6V31"
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "odYWniKR6A0r"
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "Note:\n",
        "  use Conv2d (https://pytorch.org/docs/stable/nn.html#conv2d)\n",
        "\n",
        "Note:\n",
        "  nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
        "'''\n",
        "from typing import Optional\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, \n",
        "                 nz:int, \n",
        "                 ndf:int, \n",
        "                 nc: int = 1, \n",
        "                 ngpu: Optional[int] = None):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input:  b x (nc   ) x 28 x 28\n",
        "\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # output: b x (ndf  ) x 14 x 14\n",
        "\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # output: b x (ndf*2) x 7 x 7\n",
        "\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # output: b x (ndf*4) x 3 x 3\n",
        "            \n",
        "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # output: b x (ndf*8) x 1 x 1\n",
        "            \n",
        "            nn.Conv2d(ndf * 8, 1, 1, 1, 0, bias=False),\n",
        "            # output: b x 1 x 1 x 1\n",
        "            \n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        if input.is_cuda and self.ngpu > 1:\n",
        "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
        "        else:\n",
        "            output = self.main(input)\n",
        "\n",
        "        return output.view(-1, 1).squeeze(1)\n",
        "      \n",
        "'''\n",
        "Note: \n",
        "  use ConvTranspose2d (https://pytorch.org/docs/stable/nn.html#torch.nn.ConvTranspose2d)\n",
        "\n",
        "Note:\n",
        "  torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1)\n",
        "'''\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, \n",
        "                 nz:int, \n",
        "                 ngf:int, \n",
        "                 nc: int = 1, \n",
        "                 ngpu: Optional[int] = None):\n",
        "        super(Generator, self).__init__()\n",
        "        self.ngpu = ngpu if ngpu else 1\n",
        "        self.main = nn.Sequential(\n",
        "            # input:  b x nz x 1 x 1\n",
        "            \n",
        "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            # output: b x (ngf*8) x 4 x 4\n",
        "            \n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            # output: b x (ngf*4) x 8 x 8\n",
        "            \n",
        "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 2, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # output: b x (ngf*2) x 14 x 14\n",
        "            \n",
        "            nn.ConvTranspose2d(ngf * 2, nc, 4, 2, 1, bias=False),\n",
        "            # output: b x (nc   ) x 28 x 28 \n",
        "\n",
        "            nn.Tanh()\n",
        "            # output: b x (nc   ) x 28 x 28 \n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        if input.is_cuda and self.ngpu > 1:\n",
        "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
        "        else:\n",
        "            output = self.main(input)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q5Exgup-ARNF"
      },
      "cell_type": "code",
      "source": [
        "def weights_init(m: nn.Module):\n",
        "    '''\n",
        "    custom weights initialization called on G and D\n",
        "    '''\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OktTD4d17jL7"
      },
      "cell_type": "code",
      "source": [
        "# Paths to saved model\n",
        "saved_G, saved_D = None, None\n",
        "\n",
        "# Number of GPUs\n",
        "ngpu = 1\n",
        "\n",
        "# Batch size\n",
        "batch_size = 128\n",
        "\n",
        "# How many worker to use in dataloader\n",
        "num_workers = 4\n",
        "\n",
        "# Number of channels in image from dataset\n",
        "nc = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qBkNdmkZWBtX",
        "outputId": "db0a80d7-3900-40da-8fea-5fbc9573918a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Decide which device we want to run on\n",
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
        "print(\"Running on %s\" % device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ByKeaQfp_wZI"
      },
      "cell_type": "code",
      "source": [
        "# Create data\n",
        "data = datasets.MNIST('data', download=True,\n",
        "                      transform=transforms.Compose([\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Normalize((0.5,), (0.5,)),\n",
        "                       ]))\n",
        "\n",
        "# Setup efficient dataloading\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    data, batch_size=batch_size, shuffle=True, num_workers=num_workers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "93kg_T1Y-dpW"
      },
      "cell_type": "code",
      "source": [
        "# Model parameters\n",
        "\n",
        "# latent vector size\n",
        "nz = 100\n",
        "\n",
        "# the number of filters in the discriminator\n",
        "ndf = 28\n",
        "\n",
        "# the number of filters in the generator\n",
        "ngf = 28"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RNtIGD1besb3",
        "outputId": "8553919d-c6a8-4228-d1ae-d05de065171a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "netD.cuda()\n",
        "netG.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Generator(\n",
              "  (main): Sequential(\n",
              "    (0): ConvTranspose2d(100, 224, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
              "    (1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace)\n",
              "    (3): ConvTranspose2d(224, 112, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (4): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU(inplace)\n",
              "    (6): ConvTranspose2d(112, 56, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2), bias=False)\n",
              "    (7): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (8): ReLU(inplace)\n",
              "    (9): ConvTranspose2d(56, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (10): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "metadata": {
        "id": "H3yxQBdb9geJ",
        "outputId": "1f71f3d3-55f2-471d-e9d0-152e6186d52d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "cell_type": "code",
      "source": [
        "netG = Generator(nz=nz, ngf=ngf, nc = nc, ngpu = ngpu).to(device)\n",
        "netG.apply(weights_init)\n",
        "netG.to(device)\n",
        "if saved_G is not None:\n",
        "    netG.load_state_dict(torch.load(saved_G))\n",
        "\n",
        "print(netG)\n",
        "\n",
        "netD = Discriminator(nz=nz, ndf=ndf, nc = nc, ngpu = ngpu).to(device)\n",
        "netD.apply(weights_init)\n",
        "netD.to(device)\n",
        "if saved_D is not None:\n",
        "    netD.load_state_dict(torch.load(saved_D))\n",
        "print(netD)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generator(\n",
            "  (main): Sequential(\n",
            "    (0): ConvTranspose2d(100, 224, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace)\n",
            "    (3): ConvTranspose2d(224, 112, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace)\n",
            "    (6): ConvTranspose2d(112, 56, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2), bias=False)\n",
            "    (7): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace)\n",
            "    (9): ConvTranspose2d(56, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (10): Tanh()\n",
            "  )\n",
            ")\n",
            "Discriminator(\n",
            "  (main): Sequential(\n",
            "    (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
            "    (2): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (3): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
            "    (5): Conv2d(56, 112, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (6): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): LeakyReLU(negative_slope=0.2, inplace)\n",
            "    (8): Conv2d(112, 224, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (9): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): LeakyReLU(negative_slope=0.2, inplace)\n",
            "    (11): Conv2d(224, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (12): Sigmoid()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pDqt1mNZ_dNe"
      },
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "niter = 10\n",
        "lr = 0.0002\n",
        "beta1 = 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TPvSCTjN6juZ",
        "outputId": "c496c30a-e550-44ce-f913-f93dab61b7bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8857
        }
      },
      "cell_type": "code",
      "source": [
        "# an alternative implementation of GAN objective\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "fixed_noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "real_label = 1\n",
        "fake_label = 0\n",
        "\n",
        "# setup optimizer\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "\n",
        "for epoch in range(1, niter + 1):\n",
        "    for i, (real, _) in enumerate(dataloader, 0):\n",
        "        ############################\n",
        "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "        ###########################\n",
        "        real = real.to(device)\n",
        "        \n",
        "        # train with real\n",
        "        netD.zero_grad()\n",
        "        batch_size = real.size(0)\n",
        "\n",
        "        output = netD(real)\n",
        "        D_x = output.mean().item()\n",
        "        \n",
        "        label = torch.full((batch_size,), real_label, device=device)\n",
        "        errD_real = criterion(output, label)\n",
        "        errD_real.backward()\n",
        "\n",
        "        # train with fake\n",
        "        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "        fake = netG(noise)\n",
        "\n",
        "        output = netD(fake.detach())\n",
        "        D_G_z1 = output.mean().item()\n",
        "        \n",
        "        label.fill_(fake_label)\n",
        "        errD_fake = criterion(output, label)\n",
        "        errD_fake.backward()\n",
        "        errD = errD_real + errD_fake\n",
        "        \n",
        "        optimizerD.step()\n",
        "\n",
        "        ############################\n",
        "        # (2) Update G network: maximize log(D(G(z)))\n",
        "        ###########################\n",
        "        netG.zero_grad()\n",
        "        \n",
        "        output = netD(fake)\n",
        "       \n",
        "        label.fill_(real_label)  # fake labels are real for generator cost\n",
        "        errG = criterion(output, label)\n",
        "        errG.backward()\n",
        "        \n",
        "        D_G_z2 = output.mean().item()\n",
        "        optimizerG.step()\n",
        "        \n",
        "        if i % 10 == 0:\n",
        "          print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
        "                % (epoch, niter, i, len(dataloader),\n",
        "                   errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print('[%d/%d][%d/%d] Saving samples!' \n",
        "                  % (epoch, niter, i, len(dataloader)))\n",
        "\n",
        "            vutils.save_image(real,\n",
        "                    '%s/samples_real.png' % directory,\n",
        "                    normalize=True)\n",
        "\n",
        "            fake = netG(fixed_noise)\n",
        "            vutils.save_image(fake.detach(),\n",
        "                    '%s/samples_fake_epoch_%03d_batch_%03d.png' % (directory, epoch, i),\n",
        "                    normalize=True)\n",
        "\n",
        "    # do checkpointing\n",
        "    torch.save(netG.state_dict(), '%s/netG_epoch_%d.pth' % (directory, epoch))\n",
        "    torch.save(netD.state_dict(), '%s/netD_epoch_%d.pth' % (directory, epoch))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1/10][0/469] Loss_D: 1.4072 Loss_G: 0.6584 D(x): 0.5308 D(G(z)): 0.5312 / 0.5201\n",
            "[1/10][0/469] Saving samples!\n",
            "[1/10][10/469] Loss_D: 1.3814 Loss_G: 0.7005 D(x): 0.5090 D(G(z)): 0.5040 / 0.4976\n",
            "[1/10][20/469] Loss_D: 1.3726 Loss_G: 0.7134 D(x): 0.5052 D(G(z)): 0.4966 / 0.4910\n",
            "[1/10][30/469] Loss_D: 1.3680 Loss_G: 0.7197 D(x): 0.5052 D(G(z)): 0.4938 / 0.4881\n",
            "[1/10][40/469] Loss_D: 1.3531 Loss_G: 0.7282 D(x): 0.5085 D(G(z)): 0.4900 / 0.4836\n",
            "[1/10][50/469] Loss_D: 1.3374 Loss_G: 0.7354 D(x): 0.5135 D(G(z)): 0.4872 / 0.4802\n",
            "[1/10][60/469] Loss_D: 1.3278 Loss_G: 0.7512 D(x): 0.5121 D(G(z)): 0.4804 / 0.4728\n",
            "[1/10][70/469] Loss_D: 1.3104 Loss_G: 0.7647 D(x): 0.5152 D(G(z)): 0.4745 / 0.4665\n",
            "[1/10][80/469] Loss_D: 1.2870 Loss_G: 0.7817 D(x): 0.5207 D(G(z)): 0.4671 / 0.4590\n",
            "[1/10][90/469] Loss_D: 1.2675 Loss_G: 0.7950 D(x): 0.5265 D(G(z)): 0.4623 / 0.4531\n",
            "[1/10][100/469] Loss_D: 1.2227 Loss_G: 0.8145 D(x): 0.5431 D(G(z)): 0.4549 / 0.4444\n",
            "[1/10][100/469] Saving samples!\n",
            "[1/10][110/469] Loss_D: 1.2047 Loss_G: 0.8326 D(x): 0.5456 D(G(z)): 0.4474 / 0.4361\n",
            "[1/10][120/469] Loss_D: 1.1783 Loss_G: 0.8570 D(x): 0.5499 D(G(z)): 0.4356 / 0.4263\n",
            "[1/10][130/469] Loss_D: 1.1611 Loss_G: 0.8791 D(x): 0.5516 D(G(z)): 0.4272 / 0.4170\n",
            "[1/10][140/469] Loss_D: 1.1400 Loss_G: 0.8901 D(x): 0.5623 D(G(z)): 0.4228 / 0.4141\n",
            "[1/10][150/469] Loss_D: 1.1223 Loss_G: 0.8704 D(x): 0.5830 D(G(z)): 0.4360 / 0.4208\n",
            "[1/10][160/469] Loss_D: 1.1130 Loss_G: 0.8820 D(x): 0.5851 D(G(z)): 0.4317 / 0.4184\n",
            "[1/10][170/469] Loss_D: 1.0633 Loss_G: 0.9211 D(x): 0.5929 D(G(z)): 0.4117 / 0.4017\n",
            "[1/10][180/469] Loss_D: 1.1191 Loss_G: 0.9117 D(x): 0.5724 D(G(z)): 0.4164 / 0.4066\n",
            "[1/10][190/469] Loss_D: 1.0840 Loss_G: 0.9075 D(x): 0.5933 D(G(z)): 0.4200 / 0.4076\n",
            "[1/10][200/469] Loss_D: 1.0679 Loss_G: 0.9508 D(x): 0.5935 D(G(z)): 0.4077 / 0.3918\n",
            "[1/10][200/469] Saving samples!\n",
            "[1/10][210/469] Loss_D: 1.0922 Loss_G: 0.9670 D(x): 0.5722 D(G(z)): 0.3970 / 0.3909\n",
            "[1/10][220/469] Loss_D: 1.0356 Loss_G: 0.9584 D(x): 0.6020 D(G(z)): 0.3987 / 0.3890\n",
            "[1/10][230/469] Loss_D: 1.0291 Loss_G: 0.9674 D(x): 0.6087 D(G(z)): 0.4000 / 0.3881\n",
            "[1/10][240/469] Loss_D: 1.0154 Loss_G: 0.9804 D(x): 0.6061 D(G(z)): 0.3925 / 0.3799\n",
            "[1/10][250/469] Loss_D: 1.0693 Loss_G: 1.0272 D(x): 0.5713 D(G(z)): 0.3824 / 0.3696\n",
            "[1/10][260/469] Loss_D: 0.9992 Loss_G: 1.0103 D(x): 0.6084 D(G(z)): 0.3819 / 0.3720\n",
            "[1/10][270/469] Loss_D: 0.9827 Loss_G: 1.0311 D(x): 0.6168 D(G(z)): 0.3779 / 0.3652\n",
            "[1/10][280/469] Loss_D: 0.9642 Loss_G: 1.0664 D(x): 0.6111 D(G(z)): 0.3618 / 0.3510\n",
            "[1/10][290/469] Loss_D: 0.9324 Loss_G: 1.0937 D(x): 0.6434 D(G(z)): 0.3694 / 0.3469\n",
            "[1/10][300/469] Loss_D: 0.9488 Loss_G: 1.0818 D(x): 0.6215 D(G(z)): 0.3622 / 0.3473\n",
            "[1/10][300/469] Saving samples!\n",
            "[1/10][310/469] Loss_D: 0.9545 Loss_G: 1.0772 D(x): 0.6293 D(G(z)): 0.3717 / 0.3503\n",
            "[1/10][320/469] Loss_D: 0.8552 Loss_G: 1.1357 D(x): 0.6525 D(G(z)): 0.3395 / 0.3287\n",
            "[1/10][330/469] Loss_D: 0.8425 Loss_G: 1.1258 D(x): 0.6725 D(G(z)): 0.3493 / 0.3318\n",
            "[1/10][340/469] Loss_D: 0.8463 Loss_G: 1.1505 D(x): 0.6574 D(G(z)): 0.3377 / 0.3240\n",
            "[1/10][350/469] Loss_D: 0.8211 Loss_G: 1.1715 D(x): 0.6690 D(G(z)): 0.3309 / 0.3170\n",
            "[1/10][360/469] Loss_D: 0.8182 Loss_G: 1.1664 D(x): 0.6726 D(G(z)): 0.3314 / 0.3182\n",
            "[1/10][370/469] Loss_D: 0.8078 Loss_G: 1.2254 D(x): 0.6728 D(G(z)): 0.3234 / 0.3005\n",
            "[1/10][380/469] Loss_D: 0.7837 Loss_G: 1.2513 D(x): 0.6703 D(G(z)): 0.3041 / 0.2989\n",
            "[1/10][390/469] Loss_D: 0.8258 Loss_G: 1.2290 D(x): 0.6499 D(G(z)): 0.3100 / 0.3013\n",
            "[1/10][400/469] Loss_D: 0.7621 Loss_G: 1.2467 D(x): 0.6962 D(G(z)): 0.3165 / 0.3053\n",
            "[1/10][400/469] Saving samples!\n",
            "[1/10][410/469] Loss_D: 0.7173 Loss_G: 1.2860 D(x): 0.7160 D(G(z)): 0.3068 / 0.2854\n",
            "[1/10][420/469] Loss_D: 0.8524 Loss_G: 1.2403 D(x): 0.6465 D(G(z)): 0.3123 / 0.3127\n",
            "[1/10][430/469] Loss_D: 0.7366 Loss_G: 1.3497 D(x): 0.6910 D(G(z)): 0.2845 / 0.2664\n",
            "[1/10][440/469] Loss_D: 0.7976 Loss_G: 1.3653 D(x): 0.6743 D(G(z)): 0.2841 / 0.2751\n",
            "[1/10][450/469] Loss_D: 0.7156 Loss_G: 1.3384 D(x): 0.7087 D(G(z)): 0.2986 / 0.2730\n",
            "[1/10][460/469] Loss_D: 0.7426 Loss_G: 1.2857 D(x): 0.7035 D(G(z)): 0.3068 / 0.2876\n",
            "[2/10][0/469] Loss_D: 0.6974 Loss_G: 1.3108 D(x): 0.7221 D(G(z)): 0.2908 / 0.2804\n",
            "[2/10][0/469] Saving samples!\n",
            "[2/10][10/469] Loss_D: 0.6622 Loss_G: 1.3985 D(x): 0.7381 D(G(z)): 0.2850 / 0.2567\n",
            "[2/10][20/469] Loss_D: 0.6613 Loss_G: 1.4078 D(x): 0.7174 D(G(z)): 0.2687 / 0.2592\n",
            "[2/10][30/469] Loss_D: 0.9119 Loss_G: 1.3362 D(x): 0.6098 D(G(z)): 0.2663 / 0.3089\n",
            "[2/10][40/469] Loss_D: 0.7669 Loss_G: 1.2719 D(x): 0.6893 D(G(z)): 0.2999 / 0.2963\n",
            "[2/10][50/469] Loss_D: 0.6438 Loss_G: 1.3967 D(x): 0.7523 D(G(z)): 0.2890 / 0.2565\n",
            "[2/10][60/469] Loss_D: 0.6645 Loss_G: 1.4197 D(x): 0.6996 D(G(z)): 0.2516 / 0.2538\n",
            "[2/10][70/469] Loss_D: 0.6537 Loss_G: 1.3818 D(x): 0.7399 D(G(z)): 0.2850 / 0.2625\n",
            "[2/10][80/469] Loss_D: 0.7268 Loss_G: 1.4687 D(x): 0.6958 D(G(z)): 0.2837 / 0.2457\n",
            "[2/10][90/469] Loss_D: 0.6912 Loss_G: 1.3721 D(x): 0.7382 D(G(z)): 0.3004 / 0.2673\n",
            "[2/10][100/469] Loss_D: 0.6573 Loss_G: 1.4040 D(x): 0.7253 D(G(z)): 0.2725 / 0.2575\n",
            "[2/10][100/469] Saving samples!\n",
            "[2/10][110/469] Loss_D: 0.7460 Loss_G: 1.3175 D(x): 0.6643 D(G(z)): 0.2619 / 0.2814\n",
            "[2/10][120/469] Loss_D: 0.7030 Loss_G: 1.4511 D(x): 0.7393 D(G(z)): 0.2989 / 0.2656\n",
            "[2/10][130/469] Loss_D: 0.8174 Loss_G: 1.4037 D(x): 0.6858 D(G(z)): 0.3074 / 0.2728\n",
            "[2/10][140/469] Loss_D: 0.6232 Loss_G: 1.4893 D(x): 0.7450 D(G(z)): 0.2650 / 0.2346\n",
            "[2/10][150/469] Loss_D: 0.7087 Loss_G: 1.3659 D(x): 0.7265 D(G(z)): 0.2974 / 0.2760\n",
            "[2/10][160/469] Loss_D: 0.6424 Loss_G: 1.5002 D(x): 0.7452 D(G(z)): 0.2799 / 0.2360\n",
            "[2/10][170/469] Loss_D: 0.7708 Loss_G: 1.2165 D(x): 0.6616 D(G(z)): 0.2760 / 0.3153\n",
            "[2/10][180/469] Loss_D: 0.7991 Loss_G: 1.3337 D(x): 0.6538 D(G(z)): 0.2804 / 0.2928\n",
            "[2/10][190/469] Loss_D: 0.6846 Loss_G: 1.4601 D(x): 0.6894 D(G(z)): 0.2496 / 0.2513\n",
            "[2/10][200/469] Loss_D: 0.7421 Loss_G: 1.4440 D(x): 0.7189 D(G(z)): 0.3091 / 0.2588\n",
            "[2/10][200/469] Saving samples!\n",
            "[2/10][210/469] Loss_D: 0.6790 Loss_G: 1.5302 D(x): 0.7504 D(G(z)): 0.2994 / 0.2439\n",
            "[2/10][220/469] Loss_D: 0.7693 Loss_G: 1.3925 D(x): 0.7065 D(G(z)): 0.3154 / 0.2677\n",
            "[2/10][230/469] Loss_D: 0.7919 Loss_G: 1.2588 D(x): 0.6820 D(G(z)): 0.3104 / 0.3173\n",
            "[2/10][240/469] Loss_D: 0.6968 Loss_G: 1.4998 D(x): 0.7042 D(G(z)): 0.2736 / 0.2388\n",
            "[2/10][250/469] Loss_D: 0.6649 Loss_G: 1.4281 D(x): 0.7256 D(G(z)): 0.2736 / 0.2531\n",
            "[2/10][260/469] Loss_D: 0.6891 Loss_G: 1.3788 D(x): 0.7084 D(G(z)): 0.2616 / 0.2800\n",
            "[2/10][270/469] Loss_D: 0.8022 Loss_G: 1.3725 D(x): 0.6416 D(G(z)): 0.2677 / 0.2722\n",
            "[2/10][280/469] Loss_D: 0.6446 Loss_G: 1.5613 D(x): 0.7292 D(G(z)): 0.2642 / 0.2236\n",
            "[2/10][290/469] Loss_D: 0.8715 Loss_G: 1.2767 D(x): 0.7430 D(G(z)): 0.3930 / 0.3082\n",
            "[2/10][300/469] Loss_D: 0.7765 Loss_G: 1.5317 D(x): 0.6963 D(G(z)): 0.3174 / 0.2315\n",
            "[2/10][300/469] Saving samples!\n",
            "[2/10][310/469] Loss_D: 0.7623 Loss_G: 1.5162 D(x): 0.7236 D(G(z)): 0.3300 / 0.2483\n",
            "[2/10][320/469] Loss_D: 0.8040 Loss_G: 1.1883 D(x): 0.7005 D(G(z)): 0.3409 / 0.3217\n",
            "[2/10][330/469] Loss_D: 0.7788 Loss_G: 1.4957 D(x): 0.6602 D(G(z)): 0.2714 / 0.2735\n",
            "[2/10][340/469] Loss_D: 0.6801 Loss_G: 1.3963 D(x): 0.7074 D(G(z)): 0.2686 / 0.2621\n",
            "[2/10][350/469] Loss_D: 0.9093 Loss_G: 1.4700 D(x): 0.6504 D(G(z)): 0.3369 / 0.2549\n",
            "[2/10][360/469] Loss_D: 0.6780 Loss_G: 1.5068 D(x): 0.7167 D(G(z)): 0.2754 / 0.2356\n",
            "[2/10][370/469] Loss_D: 0.6585 Loss_G: 1.4313 D(x): 0.7030 D(G(z)): 0.2314 / 0.2681\n",
            "[2/10][380/469] Loss_D: 0.7022 Loss_G: 1.5170 D(x): 0.6990 D(G(z)): 0.2664 / 0.2407\n",
            "[2/10][390/469] Loss_D: 0.7625 Loss_G: 1.3716 D(x): 0.6918 D(G(z)): 0.2980 / 0.2774\n",
            "[2/10][400/469] Loss_D: 0.6612 Loss_G: 1.4687 D(x): 0.7412 D(G(z)): 0.2769 / 0.2500\n",
            "[2/10][400/469] Saving samples!\n",
            "[2/10][410/469] Loss_D: 0.7176 Loss_G: 1.5994 D(x): 0.6776 D(G(z)): 0.2602 / 0.2148\n",
            "[2/10][420/469] Loss_D: 1.0083 Loss_G: 1.1886 D(x): 0.6365 D(G(z)): 0.3562 / 0.3491\n",
            "[2/10][430/469] Loss_D: 0.7798 Loss_G: 1.3425 D(x): 0.7243 D(G(z)): 0.3277 / 0.2853\n",
            "[2/10][440/469] Loss_D: 0.6795 Loss_G: 1.4046 D(x): 0.7329 D(G(z)): 0.2945 / 0.2637\n",
            "[2/10][450/469] Loss_D: 0.7891 Loss_G: 1.2182 D(x): 0.7005 D(G(z)): 0.3200 / 0.3106\n",
            "[2/10][460/469] Loss_D: 0.7136 Loss_G: 1.4820 D(x): 0.6912 D(G(z)): 0.2701 / 0.2444\n",
            "[3/10][0/469] Loss_D: 0.7342 Loss_G: 1.4829 D(x): 0.7357 D(G(z)): 0.3197 / 0.2521\n",
            "[3/10][0/469] Saving samples!\n",
            "[3/10][10/469] Loss_D: 0.6307 Loss_G: 1.5139 D(x): 0.7588 D(G(z)): 0.2796 / 0.2354\n",
            "[3/10][20/469] Loss_D: 0.7977 Loss_G: 1.4780 D(x): 0.6641 D(G(z)): 0.2769 / 0.2587\n",
            "[3/10][30/469] Loss_D: 0.8104 Loss_G: 1.2360 D(x): 0.7098 D(G(z)): 0.3480 / 0.3045\n",
            "[3/10][40/469] Loss_D: 0.8627 Loss_G: 1.3975 D(x): 0.6593 D(G(z)): 0.3090 / 0.2877\n",
            "[3/10][50/469] Loss_D: 0.7446 Loss_G: 1.2429 D(x): 0.7013 D(G(z)): 0.3039 / 0.3048\n",
            "[3/10][60/469] Loss_D: 0.6828 Loss_G: 1.4896 D(x): 0.7167 D(G(z)): 0.2774 / 0.2396\n",
            "[3/10][70/469] Loss_D: 0.8461 Loss_G: 1.5172 D(x): 0.5881 D(G(z)): 0.2281 / 0.2497\n",
            "[3/10][80/469] Loss_D: 0.7438 Loss_G: 1.4105 D(x): 0.6791 D(G(z)): 0.2652 / 0.2666\n",
            "[3/10][90/469] Loss_D: 0.7087 Loss_G: 1.5199 D(x): 0.7368 D(G(z)): 0.3106 / 0.2444\n",
            "[3/10][100/469] Loss_D: 0.7761 Loss_G: 1.5575 D(x): 0.6470 D(G(z)): 0.2551 / 0.2397\n",
            "[3/10][100/469] Saving samples!\n",
            "[3/10][110/469] Loss_D: 0.8444 Loss_G: 1.3704 D(x): 0.6429 D(G(z)): 0.2932 / 0.2853\n",
            "[3/10][120/469] Loss_D: 0.7048 Loss_G: 1.4640 D(x): 0.6939 D(G(z)): 0.2589 / 0.2639\n",
            "[3/10][130/469] Loss_D: 0.7986 Loss_G: 1.5278 D(x): 0.7345 D(G(z)): 0.3529 / 0.2445\n",
            "[3/10][140/469] Loss_D: 0.7497 Loss_G: 1.4086 D(x): 0.6766 D(G(z)): 0.2738 / 0.2677\n",
            "[3/10][150/469] Loss_D: 0.9889 Loss_G: 1.1352 D(x): 0.7757 D(G(z)): 0.4853 / 0.3452\n",
            "[3/10][160/469] Loss_D: 0.8408 Loss_G: 1.5730 D(x): 0.6942 D(G(z)): 0.3531 / 0.2246\n",
            "[3/10][170/469] Loss_D: 0.7544 Loss_G: 1.5247 D(x): 0.7134 D(G(z)): 0.3107 / 0.2545\n",
            "[3/10][180/469] Loss_D: 0.7622 Loss_G: 1.4835 D(x): 0.7254 D(G(z)): 0.3133 / 0.2565\n",
            "[3/10][190/469] Loss_D: 0.7942 Loss_G: 1.3518 D(x): 0.6953 D(G(z)): 0.3157 / 0.2803\n",
            "[3/10][200/469] Loss_D: 0.7530 Loss_G: 1.6681 D(x): 0.7343 D(G(z)): 0.3318 / 0.2193\n",
            "[3/10][200/469] Saving samples!\n",
            "[3/10][210/469] Loss_D: 0.7515 Loss_G: 1.3696 D(x): 0.7352 D(G(z)): 0.3270 / 0.2734\n",
            "[3/10][220/469] Loss_D: 0.8613 Loss_G: 1.0899 D(x): 0.6553 D(G(z)): 0.3130 / 0.3736\n",
            "[3/10][230/469] Loss_D: 0.7881 Loss_G: 1.3197 D(x): 0.6745 D(G(z)): 0.2972 / 0.2875\n",
            "[3/10][240/469] Loss_D: 0.8355 Loss_G: 1.3336 D(x): 0.6649 D(G(z)): 0.3066 / 0.2980\n",
            "[3/10][250/469] Loss_D: 0.7315 Loss_G: 1.5470 D(x): 0.7345 D(G(z)): 0.3064 / 0.2462\n",
            "[3/10][260/469] Loss_D: 0.7366 Loss_G: 1.6394 D(x): 0.6822 D(G(z)): 0.2622 / 0.2193\n",
            "[3/10][270/469] Loss_D: 0.6940 Loss_G: 1.4733 D(x): 0.7184 D(G(z)): 0.2783 / 0.2494\n",
            "[3/10][280/469] Loss_D: 0.8320 Loss_G: 1.1142 D(x): 0.6419 D(G(z)): 0.2778 / 0.3527\n",
            "[3/10][290/469] Loss_D: 0.6925 Loss_G: 1.5057 D(x): 0.7039 D(G(z)): 0.2595 / 0.2453\n",
            "[3/10][300/469] Loss_D: 0.7224 Loss_G: 1.4986 D(x): 0.7247 D(G(z)): 0.3043 / 0.2508\n",
            "[3/10][300/469] Saving samples!\n",
            "[3/10][310/469] Loss_D: 0.7499 Loss_G: 1.5395 D(x): 0.7097 D(G(z)): 0.3070 / 0.2361\n",
            "[3/10][320/469] Loss_D: 0.6460 Loss_G: 1.5766 D(x): 0.7481 D(G(z)): 0.2750 / 0.2261\n",
            "[3/10][330/469] Loss_D: 0.6839 Loss_G: 1.3994 D(x): 0.6917 D(G(z)): 0.2439 / 0.2732\n",
            "[3/10][340/469] Loss_D: 0.6938 Loss_G: 1.6219 D(x): 0.7293 D(G(z)): 0.2887 / 0.2218\n",
            "[3/10][350/469] Loss_D: 0.6928 Loss_G: 1.4186 D(x): 0.6885 D(G(z)): 0.2422 / 0.2644\n",
            "[3/10][360/469] Loss_D: 0.8354 Loss_G: 1.4099 D(x): 0.6713 D(G(z)): 0.3084 / 0.2985\n",
            "[3/10][370/469] Loss_D: 0.7885 Loss_G: 1.3054 D(x): 0.7008 D(G(z)): 0.3065 / 0.3083\n",
            "[3/10][380/469] Loss_D: 0.8203 Loss_G: 1.4543 D(x): 0.7566 D(G(z)): 0.3893 / 0.2537\n",
            "[3/10][390/469] Loss_D: 0.7031 Loss_G: 1.5881 D(x): 0.6982 D(G(z)): 0.2608 / 0.2244\n",
            "[3/10][400/469] Loss_D: 0.7130 Loss_G: 1.2593 D(x): 0.6495 D(G(z)): 0.2197 / 0.3105\n",
            "[3/10][400/469] Saving samples!\n",
            "[3/10][410/469] Loss_D: 0.6923 Loss_G: 1.4783 D(x): 0.7528 D(G(z)): 0.3033 / 0.2469\n",
            "[3/10][420/469] Loss_D: 0.7243 Loss_G: 1.6515 D(x): 0.7333 D(G(z)): 0.3037 / 0.2084\n",
            "[3/10][430/469] Loss_D: 0.7202 Loss_G: 1.5654 D(x): 0.6918 D(G(z)): 0.2622 / 0.2334\n",
            "[3/10][440/469] Loss_D: 0.6855 Loss_G: 1.6091 D(x): 0.6758 D(G(z)): 0.2152 / 0.2392\n",
            "[3/10][450/469] Loss_D: 0.7822 Loss_G: 1.6502 D(x): 0.6512 D(G(z)): 0.2548 / 0.2186\n",
            "[3/10][460/469] Loss_D: 0.7420 Loss_G: 1.5281 D(x): 0.6629 D(G(z)): 0.2491 / 0.2381\n",
            "[4/10][0/469] Loss_D: 0.7084 Loss_G: 1.2470 D(x): 0.6950 D(G(z)): 0.2632 / 0.3130\n",
            "[4/10][0/469] Saving samples!\n",
            "[4/10][10/469] Loss_D: 1.0412 Loss_G: 1.4727 D(x): 0.7060 D(G(z)): 0.3873 / 0.2703\n",
            "[4/10][20/469] Loss_D: 0.6451 Loss_G: 1.6275 D(x): 0.7538 D(G(z)): 0.2693 / 0.2260\n",
            "[4/10][30/469] Loss_D: 0.7608 Loss_G: 1.4333 D(x): 0.7356 D(G(z)): 0.3257 / 0.2595\n",
            "[4/10][40/469] Loss_D: 0.7538 Loss_G: 1.5098 D(x): 0.7063 D(G(z)): 0.2817 / 0.2591\n",
            "[4/10][50/469] Loss_D: 0.6808 Loss_G: 1.6529 D(x): 0.7538 D(G(z)): 0.2876 / 0.2268\n",
            "[4/10][60/469] Loss_D: 0.7009 Loss_G: 1.5376 D(x): 0.6872 D(G(z)): 0.2373 / 0.2506\n",
            "[4/10][70/469] Loss_D: 0.6543 Loss_G: 1.5293 D(x): 0.7490 D(G(z)): 0.2820 / 0.2382\n",
            "[4/10][80/469] Loss_D: 0.8445 Loss_G: 1.7898 D(x): 0.7847 D(G(z)): 0.4220 / 0.1973\n",
            "[4/10][90/469] Loss_D: 0.8091 Loss_G: 1.3618 D(x): 0.6546 D(G(z)): 0.2623 / 0.2892\n",
            "[4/10][100/469] Loss_D: 0.7791 Loss_G: 1.3573 D(x): 0.6604 D(G(z)): 0.2601 / 0.2846\n",
            "[4/10][100/469] Saving samples!\n",
            "[4/10][110/469] Loss_D: 0.9453 Loss_G: 1.1006 D(x): 0.6496 D(G(z)): 0.3073 / 0.3870\n",
            "[4/10][120/469] Loss_D: 0.7598 Loss_G: 1.6215 D(x): 0.7111 D(G(z)): 0.2965 / 0.2346\n",
            "[4/10][130/469] Loss_D: 0.7014 Loss_G: 1.5783 D(x): 0.7752 D(G(z)): 0.3238 / 0.2398\n",
            "[4/10][140/469] Loss_D: 0.7058 Loss_G: 1.5897 D(x): 0.6544 D(G(z)): 0.2035 / 0.2468\n",
            "[4/10][150/469] Loss_D: 0.8703 Loss_G: 1.5274 D(x): 0.6204 D(G(z)): 0.2743 / 0.2559\n",
            "[4/10][160/469] Loss_D: 0.7540 Loss_G: 1.9210 D(x): 0.7300 D(G(z)): 0.2987 / 0.1799\n",
            "[4/10][170/469] Loss_D: 0.9258 Loss_G: 1.1262 D(x): 0.6755 D(G(z)): 0.3627 / 0.3639\n",
            "[4/10][180/469] Loss_D: 0.7972 Loss_G: 1.6474 D(x): 0.6860 D(G(z)): 0.3050 / 0.2355\n",
            "[4/10][190/469] Loss_D: 0.7823 Loss_G: 1.6279 D(x): 0.6762 D(G(z)): 0.2730 / 0.2385\n",
            "[4/10][200/469] Loss_D: 0.6666 Loss_G: 1.4351 D(x): 0.7020 D(G(z)): 0.2393 / 0.2641\n",
            "[4/10][200/469] Saving samples!\n",
            "[4/10][210/469] Loss_D: 0.7048 Loss_G: 1.5963 D(x): 0.7238 D(G(z)): 0.2805 / 0.2488\n",
            "[4/10][220/469] Loss_D: 0.7656 Loss_G: 1.5077 D(x): 0.6777 D(G(z)): 0.2579 / 0.2522\n",
            "[4/10][230/469] Loss_D: 0.6223 Loss_G: 1.5552 D(x): 0.7644 D(G(z)): 0.2687 / 0.2409\n",
            "[4/10][240/469] Loss_D: 0.6837 Loss_G: 1.4670 D(x): 0.7705 D(G(z)): 0.3151 / 0.2585\n",
            "[4/10][250/469] Loss_D: 0.8286 Loss_G: 1.4506 D(x): 0.6252 D(G(z)): 0.2570 / 0.2599\n",
            "[4/10][260/469] Loss_D: 0.6871 Loss_G: 1.4295 D(x): 0.7344 D(G(z)): 0.2879 / 0.2691\n",
            "[4/10][270/469] Loss_D: 0.6695 Loss_G: 1.5433 D(x): 0.6820 D(G(z)): 0.2053 / 0.2341\n",
            "[4/10][280/469] Loss_D: 0.6605 Loss_G: 1.7036 D(x): 0.7579 D(G(z)): 0.2823 / 0.2102\n",
            "[4/10][290/469] Loss_D: 0.6154 Loss_G: 1.6004 D(x): 0.7755 D(G(z)): 0.2720 / 0.2268\n",
            "[4/10][300/469] Loss_D: 0.7263 Loss_G: 1.3690 D(x): 0.6880 D(G(z)): 0.2539 / 0.2934\n",
            "[4/10][300/469] Saving samples!\n",
            "[4/10][310/469] Loss_D: 0.7989 Loss_G: 1.4944 D(x): 0.7498 D(G(z)): 0.3474 / 0.2700\n",
            "[4/10][320/469] Loss_D: 0.8159 Loss_G: 1.3545 D(x): 0.6547 D(G(z)): 0.2807 / 0.2937\n",
            "[4/10][330/469] Loss_D: 0.6785 Loss_G: 1.6608 D(x): 0.7010 D(G(z)): 0.2453 / 0.2263\n",
            "[4/10][340/469] Loss_D: 0.6277 Loss_G: 1.4259 D(x): 0.7168 D(G(z)): 0.2242 / 0.2744\n",
            "[4/10][350/469] Loss_D: 0.6533 Loss_G: 1.7232 D(x): 0.7361 D(G(z)): 0.2677 / 0.2026\n",
            "[4/10][360/469] Loss_D: 0.7760 Loss_G: 1.5462 D(x): 0.7050 D(G(z)): 0.2926 / 0.2571\n",
            "[4/10][370/469] Loss_D: 0.7974 Loss_G: 1.5369 D(x): 0.6571 D(G(z)): 0.2504 / 0.2621\n",
            "[4/10][380/469] Loss_D: 0.8092 Loss_G: 1.6101 D(x): 0.7262 D(G(z)): 0.3370 / 0.2249\n",
            "[4/10][390/469] Loss_D: 0.7316 Loss_G: 1.5437 D(x): 0.7231 D(G(z)): 0.2971 / 0.2560\n",
            "[4/10][400/469] Loss_D: 0.7195 Loss_G: 1.5398 D(x): 0.6354 D(G(z)): 0.1874 / 0.2673\n",
            "[4/10][400/469] Saving samples!\n",
            "[4/10][410/469] Loss_D: 0.8546 Loss_G: 1.1400 D(x): 0.6116 D(G(z)): 0.2457 / 0.3570\n",
            "[4/10][420/469] Loss_D: 0.6427 Loss_G: 1.6743 D(x): 0.7484 D(G(z)): 0.2714 / 0.2156\n",
            "[4/10][430/469] Loss_D: 0.7935 Loss_G: 1.5586 D(x): 0.7063 D(G(z)): 0.2777 / 0.2742\n",
            "[4/10][440/469] Loss_D: 0.6852 Loss_G: 1.6873 D(x): 0.7162 D(G(z)): 0.2576 / 0.2192\n",
            "[4/10][450/469] Loss_D: 0.7239 Loss_G: 1.8340 D(x): 0.7580 D(G(z)): 0.3048 / 0.1886\n",
            "[4/10][460/469] Loss_D: 0.8168 Loss_G: 1.5009 D(x): 0.6676 D(G(z)): 0.2765 / 0.2529\n",
            "[5/10][0/469] Loss_D: 0.7488 Loss_G: 1.6042 D(x): 0.7321 D(G(z)): 0.3106 / 0.2386\n",
            "[5/10][0/469] Saving samples!\n",
            "[5/10][10/469] Loss_D: 0.7453 Loss_G: 1.6118 D(x): 0.7255 D(G(z)): 0.3086 / 0.2450\n",
            "[5/10][20/469] Loss_D: 0.7002 Loss_G: 1.6271 D(x): 0.7309 D(G(z)): 0.2834 / 0.2246\n",
            "[5/10][30/469] Loss_D: 0.5953 Loss_G: 1.7431 D(x): 0.7826 D(G(z)): 0.2613 / 0.2007\n",
            "[5/10][40/469] Loss_D: 0.6541 Loss_G: 1.6620 D(x): 0.7415 D(G(z)): 0.2740 / 0.2143\n",
            "[5/10][50/469] Loss_D: 0.7290 Loss_G: 1.3763 D(x): 0.6600 D(G(z)): 0.2137 / 0.3005\n",
            "[5/10][60/469] Loss_D: 0.7074 Loss_G: 1.5104 D(x): 0.7512 D(G(z)): 0.2958 / 0.2474\n",
            "[5/10][70/469] Loss_D: 0.7602 Loss_G: 1.5530 D(x): 0.6845 D(G(z)): 0.2603 / 0.2564\n",
            "[5/10][80/469] Loss_D: 0.6372 Loss_G: 1.6146 D(x): 0.7722 D(G(z)): 0.2909 / 0.2203\n",
            "[5/10][90/469] Loss_D: 0.7188 Loss_G: 1.6017 D(x): 0.6952 D(G(z)): 0.2465 / 0.2484\n",
            "[5/10][100/469] Loss_D: 0.8809 Loss_G: 1.7076 D(x): 0.7750 D(G(z)): 0.4239 / 0.2132\n",
            "[5/10][100/469] Saving samples!\n",
            "[5/10][110/469] Loss_D: 0.7484 Loss_G: 2.3831 D(x): 0.7667 D(G(z)): 0.3456 / 0.1347\n",
            "[5/10][120/469] Loss_D: 0.8426 Loss_G: 1.5079 D(x): 0.6550 D(G(z)): 0.2943 / 0.2537\n",
            "[5/10][130/469] Loss_D: 0.8060 Loss_G: 1.3566 D(x): 0.6857 D(G(z)): 0.3093 / 0.2833\n",
            "[5/10][140/469] Loss_D: 0.6235 Loss_G: 1.7839 D(x): 0.8094 D(G(z)): 0.3146 / 0.1872\n",
            "[5/10][150/469] Loss_D: 0.6543 Loss_G: 1.6780 D(x): 0.7033 D(G(z)): 0.2279 / 0.2197\n",
            "[5/10][160/469] Loss_D: 0.9124 Loss_G: 1.1703 D(x): 0.8475 D(G(z)): 0.4714 / 0.3357\n",
            "[5/10][170/469] Loss_D: 0.6227 Loss_G: 1.8598 D(x): 0.7684 D(G(z)): 0.2689 / 0.1823\n",
            "[5/10][180/469] Loss_D: 0.6659 Loss_G: 1.7488 D(x): 0.7460 D(G(z)): 0.2796 / 0.2033\n",
            "[5/10][190/469] Loss_D: 0.5703 Loss_G: 1.8466 D(x): 0.7962 D(G(z)): 0.2552 / 0.1883\n",
            "[5/10][200/469] Loss_D: 0.6899 Loss_G: 1.4321 D(x): 0.6473 D(G(z)): 0.1931 / 0.2743\n",
            "[5/10][200/469] Saving samples!\n",
            "[5/10][210/469] Loss_D: 0.9886 Loss_G: 1.2589 D(x): 0.7628 D(G(z)): 0.4524 / 0.3192\n",
            "[5/10][220/469] Loss_D: 0.6562 Loss_G: 1.5024 D(x): 0.7643 D(G(z)): 0.2952 / 0.2424\n",
            "[5/10][230/469] Loss_D: 0.6545 Loss_G: 1.5795 D(x): 0.7174 D(G(z)): 0.2402 / 0.2450\n",
            "[5/10][240/469] Loss_D: 0.7085 Loss_G: 1.7890 D(x): 0.7138 D(G(z)): 0.2798 / 0.1913\n",
            "[5/10][250/469] Loss_D: 0.9070 Loss_G: 1.4091 D(x): 0.8366 D(G(z)): 0.4689 / 0.2948\n",
            "[5/10][260/469] Loss_D: 0.6265 Loss_G: 1.6261 D(x): 0.7197 D(G(z)): 0.2211 / 0.2188\n",
            "[5/10][270/469] Loss_D: 0.5725 Loss_G: 1.7477 D(x): 0.7040 D(G(z)): 0.1667 / 0.1993\n",
            "[5/10][280/469] Loss_D: 0.7226 Loss_G: 1.3404 D(x): 0.6600 D(G(z)): 0.2035 / 0.3180\n",
            "[5/10][290/469] Loss_D: 0.7312 Loss_G: 1.9937 D(x): 0.7691 D(G(z)): 0.3337 / 0.1797\n",
            "[5/10][300/469] Loss_D: 0.6953 Loss_G: 1.6260 D(x): 0.7079 D(G(z)): 0.2581 / 0.2242\n",
            "[5/10][300/469] Saving samples!\n",
            "[5/10][310/469] Loss_D: 0.8191 Loss_G: 1.5675 D(x): 0.7132 D(G(z)): 0.3344 / 0.2527\n",
            "[5/10][320/469] Loss_D: 0.8967 Loss_G: 1.4555 D(x): 0.6499 D(G(z)): 0.2979 / 0.2772\n",
            "[5/10][330/469] Loss_D: 0.5324 Loss_G: 1.7621 D(x): 0.7579 D(G(z)): 0.1971 / 0.1980\n",
            "[5/10][340/469] Loss_D: 0.5368 Loss_G: 1.7142 D(x): 0.7545 D(G(z)): 0.1907 / 0.2011\n",
            "[5/10][350/469] Loss_D: 0.6421 Loss_G: 1.6354 D(x): 0.7753 D(G(z)): 0.2956 / 0.2151\n",
            "[5/10][360/469] Loss_D: 0.6977 Loss_G: 1.7071 D(x): 0.7971 D(G(z)): 0.3439 / 0.2002\n",
            "[5/10][370/469] Loss_D: 0.7240 Loss_G: 1.5751 D(x): 0.7274 D(G(z)): 0.2770 / 0.2506\n",
            "[5/10][380/469] Loss_D: 0.5838 Loss_G: 1.7715 D(x): 0.7557 D(G(z)): 0.2361 / 0.1914\n",
            "[5/10][390/469] Loss_D: 0.7801 Loss_G: 1.4145 D(x): 0.6990 D(G(z)): 0.2698 / 0.3138\n",
            "[5/10][400/469] Loss_D: 0.5946 Loss_G: 1.8075 D(x): 0.7637 D(G(z)): 0.2510 / 0.1911\n",
            "[5/10][400/469] Saving samples!\n",
            "[5/10][410/469] Loss_D: 0.5987 Loss_G: 1.8581 D(x): 0.7720 D(G(z)): 0.2534 / 0.1828\n",
            "[5/10][420/469] Loss_D: 0.7520 Loss_G: 1.4718 D(x): 0.8279 D(G(z)): 0.4015 / 0.2554\n",
            "[5/10][430/469] Loss_D: 0.5821 Loss_G: 1.9220 D(x): 0.8250 D(G(z)): 0.2930 / 0.1743\n",
            "[5/10][440/469] Loss_D: 0.8196 Loss_G: 1.2727 D(x): 0.6054 D(G(z)): 0.2262 / 0.3138\n",
            "[5/10][450/469] Loss_D: 0.5947 Loss_G: 1.9473 D(x): 0.7356 D(G(z)): 0.2141 / 0.1706\n",
            "[5/10][460/469] Loss_D: 0.5523 Loss_G: 1.9519 D(x): 0.7829 D(G(z)): 0.2295 / 0.1699\n",
            "[6/10][0/469] Loss_D: 0.8186 Loss_G: 1.4361 D(x): 0.7120 D(G(z)): 0.3245 / 0.2829\n",
            "[6/10][0/469] Saving samples!\n",
            "[6/10][10/469] Loss_D: 0.7562 Loss_G: 1.5909 D(x): 0.6876 D(G(z)): 0.2752 / 0.2284\n",
            "[6/10][20/469] Loss_D: 0.7266 Loss_G: 1.5441 D(x): 0.7698 D(G(z)): 0.3345 / 0.2555\n",
            "[6/10][30/469] Loss_D: 0.6252 Loss_G: 1.6966 D(x): 0.8221 D(G(z)): 0.3267 / 0.2086\n",
            "[6/10][40/469] Loss_D: 0.6460 Loss_G: 1.8007 D(x): 0.7193 D(G(z)): 0.2348 / 0.1963\n",
            "[6/10][50/469] Loss_D: 0.6798 Loss_G: 1.8165 D(x): 0.7386 D(G(z)): 0.2571 / 0.2023\n",
            "[6/10][60/469] Loss_D: 0.5389 Loss_G: 2.0985 D(x): 0.8265 D(G(z)): 0.2634 / 0.1590\n",
            "[6/10][70/469] Loss_D: 0.6902 Loss_G: 1.6342 D(x): 0.7600 D(G(z)): 0.2809 / 0.2354\n",
            "[6/10][80/469] Loss_D: 0.6419 Loss_G: 2.0568 D(x): 0.7919 D(G(z)): 0.3099 / 0.1519\n",
            "[6/10][90/469] Loss_D: 0.8822 Loss_G: 1.5016 D(x): 0.8196 D(G(z)): 0.4463 / 0.2811\n",
            "[6/10][100/469] Loss_D: 0.5536 Loss_G: 1.7796 D(x): 0.7950 D(G(z)): 0.2480 / 0.1964\n",
            "[6/10][100/469] Saving samples!\n",
            "[6/10][110/469] Loss_D: 0.5698 Loss_G: 1.6675 D(x): 0.7810 D(G(z)): 0.2535 / 0.2078\n",
            "[6/10][120/469] Loss_D: 0.6286 Loss_G: 1.9386 D(x): 0.7859 D(G(z)): 0.2852 / 0.1835\n",
            "[6/10][130/469] Loss_D: 0.5220 Loss_G: 2.2656 D(x): 0.8550 D(G(z)): 0.2732 / 0.1486\n",
            "[6/10][140/469] Loss_D: 0.5414 Loss_G: 1.9362 D(x): 0.8064 D(G(z)): 0.2518 / 0.1701\n",
            "[6/10][150/469] Loss_D: 0.7285 Loss_G: 1.4643 D(x): 0.7025 D(G(z)): 0.2751 / 0.2615\n",
            "[6/10][160/469] Loss_D: 0.7408 Loss_G: 1.5679 D(x): 0.6637 D(G(z)): 0.2277 / 0.2366\n",
            "[6/10][170/469] Loss_D: 0.5200 Loss_G: 2.1089 D(x): 0.8356 D(G(z)): 0.2627 / 0.1435\n",
            "[6/10][180/469] Loss_D: 0.6225 Loss_G: 1.6870 D(x): 0.7286 D(G(z)): 0.2374 / 0.2052\n",
            "[6/10][190/469] Loss_D: 0.5788 Loss_G: 1.6853 D(x): 0.7682 D(G(z)): 0.2414 / 0.2118\n",
            "[6/10][200/469] Loss_D: 0.6768 Loss_G: 1.7800 D(x): 0.7603 D(G(z)): 0.2952 / 0.2001\n",
            "[6/10][200/469] Saving samples!\n",
            "[6/10][210/469] Loss_D: 0.7403 Loss_G: 2.3300 D(x): 0.7871 D(G(z)): 0.3380 / 0.1512\n",
            "[6/10][220/469] Loss_D: 0.7090 Loss_G: 1.4768 D(x): 0.7035 D(G(z)): 0.2514 / 0.2607\n",
            "[6/10][230/469] Loss_D: 0.6615 Loss_G: 1.5265 D(x): 0.6920 D(G(z)): 0.2184 / 0.2422\n",
            "[6/10][240/469] Loss_D: 0.6403 Loss_G: 1.7614 D(x): 0.7829 D(G(z)): 0.2878 / 0.1985\n",
            "[6/10][250/469] Loss_D: 0.6550 Loss_G: 1.6254 D(x): 0.6740 D(G(z)): 0.2006 / 0.2271\n",
            "[6/10][260/469] Loss_D: 0.8389 Loss_G: 1.1782 D(x): 0.5383 D(G(z)): 0.1401 / 0.3603\n",
            "[6/10][270/469] Loss_D: 0.8115 Loss_G: 1.5133 D(x): 0.7499 D(G(z)): 0.3527 / 0.2646\n",
            "[6/10][280/469] Loss_D: 0.6342 Loss_G: 1.4772 D(x): 0.7775 D(G(z)): 0.2872 / 0.2701\n",
            "[6/10][290/469] Loss_D: 0.6972 Loss_G: 1.6552 D(x): 0.7183 D(G(z)): 0.2648 / 0.2188\n",
            "[6/10][300/469] Loss_D: 0.6362 Loss_G: 1.8068 D(x): 0.7363 D(G(z)): 0.2511 / 0.1895\n",
            "[6/10][300/469] Saving samples!\n",
            "[6/10][310/469] Loss_D: 0.8623 Loss_G: 1.7687 D(x): 0.6378 D(G(z)): 0.2505 / 0.2421\n",
            "[6/10][320/469] Loss_D: 0.6177 Loss_G: 1.6010 D(x): 0.7418 D(G(z)): 0.2423 / 0.2221\n",
            "[6/10][330/469] Loss_D: 0.9376 Loss_G: 0.9867 D(x): 0.5709 D(G(z)): 0.2362 / 0.4135\n",
            "[6/10][340/469] Loss_D: 0.8607 Loss_G: 1.2876 D(x): 0.6096 D(G(z)): 0.2444 / 0.3372\n",
            "[6/10][350/469] Loss_D: 0.6663 Loss_G: 1.6753 D(x): 0.7631 D(G(z)): 0.2702 / 0.2248\n",
            "[6/10][360/469] Loss_D: 0.7998 Loss_G: 1.2422 D(x): 0.5800 D(G(z)): 0.1719 / 0.3274\n",
            "[6/10][370/469] Loss_D: 0.5574 Loss_G: 2.0213 D(x): 0.7925 D(G(z)): 0.2409 / 0.1780\n",
            "[6/10][380/469] Loss_D: 0.5925 Loss_G: 2.0643 D(x): 0.8484 D(G(z)): 0.3170 / 0.1486\n",
            "[6/10][390/469] Loss_D: 0.6845 Loss_G: 1.5203 D(x): 0.6738 D(G(z)): 0.2097 / 0.2453\n",
            "[6/10][400/469] Loss_D: 0.6224 Loss_G: 1.8922 D(x): 0.7689 D(G(z)): 0.2528 / 0.1957\n",
            "[6/10][400/469] Saving samples!\n",
            "[6/10][410/469] Loss_D: 0.9317 Loss_G: 1.4912 D(x): 0.5487 D(G(z)): 0.1923 / 0.2723\n",
            "[6/10][420/469] Loss_D: 0.5569 Loss_G: 1.8351 D(x): 0.7947 D(G(z)): 0.2511 / 0.1939\n",
            "[6/10][430/469] Loss_D: 0.6823 Loss_G: 1.4836 D(x): 0.7019 D(G(z)): 0.2415 / 0.2763\n",
            "[6/10][440/469] Loss_D: 0.8398 Loss_G: 1.7829 D(x): 0.6423 D(G(z)): 0.2395 / 0.2100\n",
            "[6/10][450/469] Loss_D: 0.6232 Loss_G: 1.7735 D(x): 0.6942 D(G(z)): 0.1802 / 0.2050\n",
            "[6/10][460/469] Loss_D: 1.1679 Loss_G: 2.0890 D(x): 0.8023 D(G(z)): 0.4581 / 0.1665\n",
            "[7/10][0/469] Loss_D: 0.8143 Loss_G: 1.2449 D(x): 0.6353 D(G(z)): 0.2589 / 0.3151\n",
            "[7/10][0/469] Saving samples!\n",
            "[7/10][10/469] Loss_D: 0.8415 Loss_G: 0.9051 D(x): 0.5626 D(G(z)): 0.1630 / 0.4449\n",
            "[7/10][20/469] Loss_D: 0.5657 Loss_G: 1.9194 D(x): 0.8593 D(G(z)): 0.2907 / 0.1756\n",
            "[7/10][30/469] Loss_D: 0.6659 Loss_G: 2.0272 D(x): 0.8272 D(G(z)): 0.3388 / 0.1654\n",
            "[7/10][40/469] Loss_D: 1.0927 Loss_G: 1.1069 D(x): 0.5591 D(G(z)): 0.3067 / 0.3975\n",
            "[7/10][50/469] Loss_D: 0.7290 Loss_G: 1.8282 D(x): 0.7482 D(G(z)): 0.3053 / 0.1915\n",
            "[7/10][60/469] Loss_D: 0.6244 Loss_G: 1.7465 D(x): 0.7060 D(G(z)): 0.2034 / 0.2115\n",
            "[7/10][70/469] Loss_D: 0.6392 Loss_G: 2.0383 D(x): 0.7839 D(G(z)): 0.2949 / 0.1565\n",
            "[7/10][80/469] Loss_D: 0.5898 Loss_G: 2.0562 D(x): 0.8109 D(G(z)): 0.2887 / 0.1547\n",
            "[7/10][90/469] Loss_D: 0.6905 Loss_G: 1.9048 D(x): 0.7703 D(G(z)): 0.3089 / 0.1852\n",
            "[7/10][100/469] Loss_D: 0.5443 Loss_G: 2.1368 D(x): 0.7860 D(G(z)): 0.2335 / 0.1385\n",
            "[7/10][100/469] Saving samples!\n",
            "[7/10][110/469] Loss_D: 0.8399 Loss_G: 1.9282 D(x): 0.7162 D(G(z)): 0.3481 / 0.1796\n",
            "[7/10][120/469] Loss_D: 0.7817 Loss_G: 2.1099 D(x): 0.8740 D(G(z)): 0.4204 / 0.1657\n",
            "[7/10][130/469] Loss_D: 0.7155 Loss_G: 1.8710 D(x): 0.6900 D(G(z)): 0.2570 / 0.1856\n",
            "[7/10][140/469] Loss_D: 0.5918 Loss_G: 1.8474 D(x): 0.7400 D(G(z)): 0.2151 / 0.1888\n",
            "[7/10][150/469] Loss_D: 0.7782 Loss_G: 1.2772 D(x): 0.5928 D(G(z)): 0.1490 / 0.3433\n",
            "[7/10][160/469] Loss_D: 0.5315 Loss_G: 1.6627 D(x): 0.8494 D(G(z)): 0.2781 / 0.2172\n",
            "[7/10][170/469] Loss_D: 0.6009 Loss_G: 1.6688 D(x): 0.7866 D(G(z)): 0.2740 / 0.2302\n",
            "[7/10][180/469] Loss_D: 0.6162 Loss_G: 1.6316 D(x): 0.7018 D(G(z)): 0.1848 / 0.2399\n",
            "[7/10][190/469] Loss_D: 0.5990 Loss_G: 1.6394 D(x): 0.8024 D(G(z)): 0.2878 / 0.2241\n",
            "[7/10][200/469] Loss_D: 0.6174 Loss_G: 1.7992 D(x): 0.7438 D(G(z)): 0.2223 / 0.2159\n",
            "[7/10][200/469] Saving samples!\n",
            "[7/10][210/469] Loss_D: 0.6880 Loss_G: 1.7219 D(x): 0.7022 D(G(z)): 0.2166 / 0.2183\n",
            "[7/10][220/469] Loss_D: 0.6385 Loss_G: 1.7377 D(x): 0.7044 D(G(z)): 0.2146 / 0.2004\n",
            "[7/10][230/469] Loss_D: 0.6605 Loss_G: 1.9534 D(x): 0.7218 D(G(z)): 0.2386 / 0.1823\n",
            "[7/10][240/469] Loss_D: 0.7389 Loss_G: 1.4702 D(x): 0.7200 D(G(z)): 0.2647 / 0.2678\n",
            "[7/10][250/469] Loss_D: 0.4759 Loss_G: 1.9085 D(x): 0.8023 D(G(z)): 0.2001 / 0.1772\n",
            "[7/10][260/469] Loss_D: 0.8845 Loss_G: 1.7413 D(x): 0.8592 D(G(z)): 0.4653 / 0.2122\n",
            "[7/10][270/469] Loss_D: 0.6250 Loss_G: 1.6524 D(x): 0.7740 D(G(z)): 0.2679 / 0.2205\n",
            "[7/10][280/469] Loss_D: 0.5874 Loss_G: 1.6638 D(x): 0.7162 D(G(z)): 0.1851 / 0.2289\n",
            "[7/10][290/469] Loss_D: 0.5522 Loss_G: 1.7286 D(x): 0.7795 D(G(z)): 0.2337 / 0.2053\n",
            "[7/10][300/469] Loss_D: 0.5966 Loss_G: 1.5620 D(x): 0.7438 D(G(z)): 0.2311 / 0.2385\n",
            "[7/10][300/469] Saving samples!\n",
            "[7/10][310/469] Loss_D: 0.7265 Loss_G: 1.6130 D(x): 0.6409 D(G(z)): 0.1809 / 0.2710\n",
            "[7/10][320/469] Loss_D: 0.5126 Loss_G: 1.7377 D(x): 0.7473 D(G(z)): 0.1730 / 0.2001\n",
            "[7/10][330/469] Loss_D: 0.5222 Loss_G: 1.9903 D(x): 0.7815 D(G(z)): 0.2088 / 0.1721\n",
            "[7/10][340/469] Loss_D: 0.5585 Loss_G: 1.6844 D(x): 0.7578 D(G(z)): 0.1964 / 0.2308\n",
            "[7/10][350/469] Loss_D: 0.5443 Loss_G: 1.8133 D(x): 0.8296 D(G(z)): 0.2712 / 0.1916\n",
            "[7/10][360/469] Loss_D: 0.6217 Loss_G: 1.8055 D(x): 0.7307 D(G(z)): 0.2275 / 0.1891\n",
            "[7/10][370/469] Loss_D: 0.9543 Loss_G: 1.5494 D(x): 0.6314 D(G(z)): 0.2972 / 0.2587\n",
            "[7/10][380/469] Loss_D: 0.7577 Loss_G: 1.5225 D(x): 0.7439 D(G(z)): 0.3257 / 0.2461\n",
            "[7/10][390/469] Loss_D: 0.5992 Loss_G: 2.1326 D(x): 0.8421 D(G(z)): 0.3233 / 0.1414\n",
            "[7/10][400/469] Loss_D: 0.6090 Loss_G: 1.7784 D(x): 0.6870 D(G(z)): 0.1718 / 0.2033\n",
            "[7/10][400/469] Saving samples!\n",
            "[7/10][410/469] Loss_D: 0.8731 Loss_G: 1.7582 D(x): 0.6995 D(G(z)): 0.3133 / 0.2318\n",
            "[7/10][420/469] Loss_D: 0.7150 Loss_G: 1.4696 D(x): 0.6529 D(G(z)): 0.1709 / 0.2739\n",
            "[7/10][430/469] Loss_D: 0.6860 Loss_G: 1.6485 D(x): 0.7269 D(G(z)): 0.2570 / 0.2378\n",
            "[7/10][440/469] Loss_D: 0.6274 Loss_G: 1.7196 D(x): 0.6916 D(G(z)): 0.1897 / 0.2144\n",
            "[7/10][450/469] Loss_D: 0.5908 Loss_G: 2.4867 D(x): 0.8359 D(G(z)): 0.3082 / 0.1068\n",
            "[7/10][460/469] Loss_D: 0.6653 Loss_G: 2.8795 D(x): 0.8365 D(G(z)): 0.3336 / 0.1022\n",
            "[8/10][0/469] Loss_D: 0.8395 Loss_G: 1.5486 D(x): 0.6831 D(G(z)): 0.3002 / 0.2556\n",
            "[8/10][0/469] Saving samples!\n",
            "[8/10][10/469] Loss_D: 0.6923 Loss_G: 1.7539 D(x): 0.7095 D(G(z)): 0.2482 / 0.2108\n",
            "[8/10][20/469] Loss_D: 0.5901 Loss_G: 1.7934 D(x): 0.7802 D(G(z)): 0.2520 / 0.2048\n",
            "[8/10][30/469] Loss_D: 0.6873 Loss_G: 1.7855 D(x): 0.7406 D(G(z)): 0.2747 / 0.1941\n",
            "[8/10][40/469] Loss_D: 0.8487 Loss_G: 1.4599 D(x): 0.5735 D(G(z)): 0.1609 / 0.2949\n",
            "[8/10][50/469] Loss_D: 0.5818 Loss_G: 1.8298 D(x): 0.8059 D(G(z)): 0.2737 / 0.1968\n",
            "[8/10][60/469] Loss_D: 0.5062 Loss_G: 1.9707 D(x): 0.7681 D(G(z)): 0.1751 / 0.1691\n",
            "[8/10][70/469] Loss_D: 0.7557 Loss_G: 1.7378 D(x): 0.7011 D(G(z)): 0.2822 / 0.2179\n",
            "[8/10][80/469] Loss_D: 0.7005 Loss_G: 1.7435 D(x): 0.8051 D(G(z)): 0.3085 / 0.2064\n",
            "[8/10][90/469] Loss_D: 0.5932 Loss_G: 1.8728 D(x): 0.7727 D(G(z)): 0.2544 / 0.1794\n",
            "[8/10][100/469] Loss_D: 0.6217 Loss_G: 1.8483 D(x): 0.7733 D(G(z)): 0.2715 / 0.1851\n",
            "[8/10][100/469] Saving samples!\n",
            "[8/10][110/469] Loss_D: 0.8437 Loss_G: 1.3606 D(x): 0.8602 D(G(z)): 0.4536 / 0.2995\n",
            "[8/10][120/469] Loss_D: 0.9045 Loss_G: 1.5128 D(x): 0.6416 D(G(z)): 0.3027 / 0.2492\n",
            "[8/10][130/469] Loss_D: 0.8359 Loss_G: 1.3901 D(x): 0.5946 D(G(z)): 0.2121 / 0.2962\n",
            "[8/10][140/469] Loss_D: 0.6051 Loss_G: 1.6852 D(x): 0.7782 D(G(z)): 0.2632 / 0.2157\n",
            "[8/10][150/469] Loss_D: 0.4227 Loss_G: 2.0579 D(x): 0.8538 D(G(z)): 0.2157 / 0.1499\n",
            "[8/10][160/469] Loss_D: 0.6823 Loss_G: 1.4274 D(x): 0.6669 D(G(z)): 0.1872 / 0.2806\n",
            "[8/10][170/469] Loss_D: 0.6745 Loss_G: 1.8964 D(x): 0.7304 D(G(z)): 0.2439 / 0.1852\n",
            "[8/10][180/469] Loss_D: 0.8437 Loss_G: 1.6072 D(x): 0.6826 D(G(z)): 0.3038 / 0.2350\n",
            "[8/10][190/469] Loss_D: 0.6562 Loss_G: 1.7184 D(x): 0.7070 D(G(z)): 0.2272 / 0.2245\n",
            "[8/10][200/469] Loss_D: 0.7709 Loss_G: 0.8908 D(x): 0.6295 D(G(z)): 0.2203 / 0.4468\n",
            "[8/10][200/469] Saving samples!\n",
            "[8/10][210/469] Loss_D: 0.9024 Loss_G: 1.6389 D(x): 0.7377 D(G(z)): 0.3815 / 0.2897\n",
            "[8/10][220/469] Loss_D: 0.5338 Loss_G: 1.9505 D(x): 0.8046 D(G(z)): 0.2239 / 0.1780\n",
            "[8/10][230/469] Loss_D: 0.6819 Loss_G: 1.3641 D(x): 0.6571 D(G(z)): 0.1966 / 0.3016\n",
            "[8/10][240/469] Loss_D: 0.8069 Loss_G: 1.5040 D(x): 0.7129 D(G(z)): 0.3211 / 0.2543\n",
            "[8/10][250/469] Loss_D: 0.6188 Loss_G: 1.7693 D(x): 0.7610 D(G(z)): 0.2457 / 0.2064\n",
            "[8/10][260/469] Loss_D: 0.6724 Loss_G: 2.0429 D(x): 0.7528 D(G(z)): 0.2804 / 0.1569\n",
            "[8/10][270/469] Loss_D: 0.6465 Loss_G: 1.5041 D(x): 0.6815 D(G(z)): 0.1868 / 0.2571\n",
            "[8/10][280/469] Loss_D: 0.6078 Loss_G: 1.8328 D(x): 0.7799 D(G(z)): 0.2573 / 0.1931\n",
            "[8/10][290/469] Loss_D: 0.7084 Loss_G: 1.9515 D(x): 0.7920 D(G(z)): 0.3421 / 0.1724\n",
            "[8/10][300/469] Loss_D: 0.7563 Loss_G: 1.4658 D(x): 0.6701 D(G(z)): 0.2331 / 0.2798\n",
            "[8/10][300/469] Saving samples!\n",
            "[8/10][310/469] Loss_D: 0.7416 Loss_G: 1.6853 D(x): 0.7688 D(G(z)): 0.3349 / 0.2196\n",
            "[8/10][320/469] Loss_D: 0.6476 Loss_G: 1.6466 D(x): 0.7060 D(G(z)): 0.2182 / 0.2353\n",
            "[8/10][330/469] Loss_D: 0.6573 Loss_G: 1.8209 D(x): 0.7573 D(G(z)): 0.2572 / 0.1911\n",
            "[8/10][340/469] Loss_D: 0.9704 Loss_G: 1.5690 D(x): 0.6110 D(G(z)): 0.2325 / 0.2737\n",
            "[8/10][350/469] Loss_D: 0.5655 Loss_G: 1.9370 D(x): 0.7381 D(G(z)): 0.1901 / 0.1898\n",
            "[8/10][360/469] Loss_D: 0.5312 Loss_G: 1.7494 D(x): 0.8221 D(G(z)): 0.2636 / 0.1940\n",
            "[8/10][370/469] Loss_D: 1.2092 Loss_G: 2.5418 D(x): 0.8645 D(G(z)): 0.5973 / 0.1198\n",
            "[8/10][380/469] Loss_D: 0.7380 Loss_G: 1.5075 D(x): 0.6749 D(G(z)): 0.2397 / 0.2638\n",
            "[8/10][390/469] Loss_D: 0.6915 Loss_G: 1.8767 D(x): 0.7962 D(G(z)): 0.3242 / 0.1872\n",
            "[8/10][400/469] Loss_D: 0.5849 Loss_G: 1.8413 D(x): 0.7739 D(G(z)): 0.2378 / 0.1920\n",
            "[8/10][400/469] Saving samples!\n",
            "[8/10][410/469] Loss_D: 0.6167 Loss_G: 1.9048 D(x): 0.8193 D(G(z)): 0.3082 / 0.1755\n",
            "[8/10][420/469] Loss_D: 0.6977 Loss_G: 1.9693 D(x): 0.7482 D(G(z)): 0.2331 / 0.1784\n",
            "[8/10][430/469] Loss_D: 0.4944 Loss_G: 2.1130 D(x): 0.7219 D(G(z)): 0.1288 / 0.1559\n",
            "[8/10][440/469] Loss_D: 0.9903 Loss_G: 1.2477 D(x): 0.5790 D(G(z)): 0.2224 / 0.3555\n",
            "[8/10][450/469] Loss_D: 0.5536 Loss_G: 1.5905 D(x): 0.7887 D(G(z)): 0.2472 / 0.2225\n",
            "[8/10][460/469] Loss_D: 0.9369 Loss_G: 1.4995 D(x): 0.4953 D(G(z)): 0.1381 / 0.2644\n",
            "[9/10][0/469] Loss_D: 0.6834 Loss_G: 1.8907 D(x): 0.7207 D(G(z)): 0.2548 / 0.1904\n",
            "[9/10][0/469] Saving samples!\n",
            "[9/10][10/469] Loss_D: 0.4983 Loss_G: 2.0602 D(x): 0.7567 D(G(z)): 0.1606 / 0.1536\n",
            "[9/10][20/469] Loss_D: 0.7050 Loss_G: 1.9301 D(x): 0.7081 D(G(z)): 0.2440 / 0.1800\n",
            "[9/10][30/469] Loss_D: 0.4930 Loss_G: 2.2152 D(x): 0.8459 D(G(z)): 0.2513 / 0.1387\n",
            "[9/10][40/469] Loss_D: 0.5360 Loss_G: 1.8854 D(x): 0.8699 D(G(z)): 0.3019 / 0.1724\n",
            "[9/10][50/469] Loss_D: 0.5585 Loss_G: 1.9184 D(x): 0.6839 D(G(z)): 0.1210 / 0.1835\n",
            "[9/10][60/469] Loss_D: 0.5195 Loss_G: 1.7943 D(x): 0.7568 D(G(z)): 0.1815 / 0.2025\n",
            "[9/10][70/469] Loss_D: 0.6776 Loss_G: 1.7522 D(x): 0.6846 D(G(z)): 0.1975 / 0.2213\n",
            "[9/10][80/469] Loss_D: 0.8077 Loss_G: 1.4932 D(x): 0.7544 D(G(z)): 0.3502 / 0.2566\n",
            "[9/10][90/469] Loss_D: 0.8250 Loss_G: 2.5173 D(x): 0.8163 D(G(z)): 0.4057 / 0.1185\n",
            "[9/10][100/469] Loss_D: 0.7387 Loss_G: 1.8373 D(x): 0.8621 D(G(z)): 0.3938 / 0.2126\n",
            "[9/10][100/469] Saving samples!\n",
            "[9/10][110/469] Loss_D: 0.4786 Loss_G: 1.7394 D(x): 0.8471 D(G(z)): 0.2447 / 0.2013\n",
            "[9/10][120/469] Loss_D: 0.7391 Loss_G: 1.7125 D(x): 0.7318 D(G(z)): 0.2960 / 0.2156\n",
            "[9/10][130/469] Loss_D: 0.6005 Loss_G: 1.7807 D(x): 0.7857 D(G(z)): 0.2722 / 0.1961\n",
            "[9/10][140/469] Loss_D: 0.8819 Loss_G: 1.7172 D(x): 0.8375 D(G(z)): 0.4395 / 0.2182\n",
            "[9/10][150/469] Loss_D: 0.9052 Loss_G: 1.6925 D(x): 0.7645 D(G(z)): 0.4000 / 0.2448\n",
            "[9/10][160/469] Loss_D: 0.6384 Loss_G: 1.6720 D(x): 0.7722 D(G(z)): 0.2750 / 0.2203\n",
            "[9/10][170/469] Loss_D: 0.7185 Loss_G: 1.7803 D(x): 0.7132 D(G(z)): 0.2629 / 0.2115\n",
            "[9/10][180/469] Loss_D: 0.5514 Loss_G: 1.7093 D(x): 0.7428 D(G(z)): 0.1925 / 0.2215\n",
            "[9/10][190/469] Loss_D: 0.7244 Loss_G: 2.0019 D(x): 0.8492 D(G(z)): 0.3565 / 0.2050\n",
            "[9/10][200/469] Loss_D: 0.5291 Loss_G: 2.0711 D(x): 0.8387 D(G(z)): 0.2668 / 0.1540\n",
            "[9/10][200/469] Saving samples!\n",
            "[9/10][210/469] Loss_D: 0.6320 Loss_G: 2.0859 D(x): 0.7970 D(G(z)): 0.2982 / 0.1504\n",
            "[9/10][220/469] Loss_D: 0.5779 Loss_G: 2.0069 D(x): 0.7182 D(G(z)): 0.1833 / 0.1679\n",
            "[9/10][230/469] Loss_D: 0.5416 Loss_G: 1.5957 D(x): 0.7318 D(G(z)): 0.1506 / 0.2420\n",
            "[9/10][240/469] Loss_D: 0.5574 Loss_G: 2.0951 D(x): 0.8303 D(G(z)): 0.2636 / 0.1535\n",
            "[9/10][250/469] Loss_D: 0.9245 Loss_G: 1.1193 D(x): 0.5666 D(G(z)): 0.2248 / 0.3719\n",
            "[9/10][260/469] Loss_D: 0.4171 Loss_G: 2.3163 D(x): 0.8373 D(G(z)): 0.1927 / 0.1153\n",
            "[9/10][270/469] Loss_D: 0.7816 Loss_G: 1.6153 D(x): 0.6695 D(G(z)): 0.2345 / 0.2509\n",
            "[9/10][280/469] Loss_D: 0.6365 Loss_G: 1.8609 D(x): 0.7637 D(G(z)): 0.2675 / 0.1899\n",
            "[9/10][290/469] Loss_D: 0.6711 Loss_G: 2.1785 D(x): 0.8707 D(G(z)): 0.3793 / 0.1452\n",
            "[9/10][300/469] Loss_D: 0.7871 Loss_G: 1.9082 D(x): 0.7546 D(G(z)): 0.3330 / 0.1958\n",
            "[9/10][300/469] Saving samples!\n",
            "[9/10][310/469] Loss_D: 0.6340 Loss_G: 1.7557 D(x): 0.7494 D(G(z)): 0.2536 / 0.1938\n",
            "[9/10][320/469] Loss_D: 0.6185 Loss_G: 1.7524 D(x): 0.8110 D(G(z)): 0.2953 / 0.2059\n",
            "[9/10][330/469] Loss_D: 0.6471 Loss_G: 2.4145 D(x): 0.8240 D(G(z)): 0.3128 / 0.1416\n",
            "[9/10][340/469] Loss_D: 0.6590 Loss_G: 2.0666 D(x): 0.8253 D(G(z)): 0.3270 / 0.1597\n",
            "[9/10][350/469] Loss_D: 0.6601 Loss_G: 1.9124 D(x): 0.7765 D(G(z)): 0.2936 / 0.1731\n",
            "[9/10][360/469] Loss_D: 0.4978 Loss_G: 2.0996 D(x): 0.8203 D(G(z)): 0.2378 / 0.1393\n",
            "[9/10][370/469] Loss_D: 0.5753 Loss_G: 2.4665 D(x): 0.7973 D(G(z)): 0.2589 / 0.1094\n",
            "[9/10][380/469] Loss_D: 0.7129 Loss_G: 1.7452 D(x): 0.7204 D(G(z)): 0.2703 / 0.2106\n",
            "[9/10][390/469] Loss_D: 0.6866 Loss_G: 1.7766 D(x): 0.7230 D(G(z)): 0.2304 / 0.2259\n",
            "[9/10][400/469] Loss_D: 0.5672 Loss_G: 2.2412 D(x): 0.7793 D(G(z)): 0.2130 / 0.1464\n",
            "[9/10][400/469] Saving samples!\n",
            "[9/10][410/469] Loss_D: 0.6270 Loss_G: 1.9459 D(x): 0.7749 D(G(z)): 0.2721 / 0.1719\n",
            "[9/10][420/469] Loss_D: 0.5668 Loss_G: 1.8796 D(x): 0.7198 D(G(z)): 0.1786 / 0.1853\n",
            "[9/10][430/469] Loss_D: 0.5831 Loss_G: 1.8780 D(x): 0.7777 D(G(z)): 0.2549 / 0.1959\n",
            "[9/10][440/469] Loss_D: 0.7169 Loss_G: 2.1412 D(x): 0.8204 D(G(z)): 0.3431 / 0.1546\n",
            "[9/10][450/469] Loss_D: 0.8381 Loss_G: 1.5095 D(x): 0.6404 D(G(z)): 0.2737 / 0.2592\n",
            "[9/10][460/469] Loss_D: 0.5880 Loss_G: 2.1798 D(x): 0.8548 D(G(z)): 0.3129 / 0.1401\n",
            "[10/10][0/469] Loss_D: 0.6787 Loss_G: 1.9549 D(x): 0.7962 D(G(z)): 0.3112 / 0.1728\n",
            "[10/10][0/469] Saving samples!\n",
            "[10/10][10/469] Loss_D: 0.6150 Loss_G: 2.3104 D(x): 0.7850 D(G(z)): 0.2550 / 0.1317\n",
            "[10/10][20/469] Loss_D: 1.0553 Loss_G: 1.3469 D(x): 0.5409 D(G(z)): 0.2609 / 0.3274\n",
            "[10/10][30/469] Loss_D: 0.6060 Loss_G: 1.7271 D(x): 0.8442 D(G(z)): 0.3223 / 0.1983\n",
            "[10/10][40/469] Loss_D: 0.4936 Loss_G: 2.1761 D(x): 0.8407 D(G(z)): 0.2480 / 0.1415\n",
            "[10/10][50/469] Loss_D: 0.6041 Loss_G: 2.0586 D(x): 0.7024 D(G(z)): 0.1672 / 0.1844\n",
            "[10/10][60/469] Loss_D: 0.7731 Loss_G: 1.2887 D(x): 0.6232 D(G(z)): 0.1951 / 0.3273\n",
            "[10/10][70/469] Loss_D: 0.6582 Loss_G: 2.0474 D(x): 0.7657 D(G(z)): 0.2872 / 0.1627\n",
            "[10/10][80/469] Loss_D: 0.5651 Loss_G: 1.7573 D(x): 0.7734 D(G(z)): 0.2337 / 0.2049\n",
            "[10/10][90/469] Loss_D: 0.5794 Loss_G: 1.7390 D(x): 0.7452 D(G(z)): 0.2181 / 0.2084\n",
            "[10/10][100/469] Loss_D: 0.7629 Loss_G: 2.3971 D(x): 0.8117 D(G(z)): 0.3757 / 0.1212\n",
            "[10/10][100/469] Saving samples!\n",
            "[10/10][110/469] Loss_D: 0.4537 Loss_G: 1.9982 D(x): 0.8426 D(G(z)): 0.2246 / 0.1564\n",
            "[10/10][120/469] Loss_D: 0.5697 Loss_G: 1.8466 D(x): 0.6930 D(G(z)): 0.1503 / 0.1926\n",
            "[10/10][130/469] Loss_D: 0.6085 Loss_G: 2.2957 D(x): 0.8424 D(G(z)): 0.3093 / 0.1591\n",
            "[10/10][140/469] Loss_D: 0.6901 Loss_G: 1.5745 D(x): 0.6720 D(G(z)): 0.1961 / 0.2473\n",
            "[10/10][150/469] Loss_D: 0.8319 Loss_G: 2.6802 D(x): 0.8538 D(G(z)): 0.4385 / 0.1087\n",
            "[10/10][160/469] Loss_D: 0.6658 Loss_G: 1.4509 D(x): 0.7331 D(G(z)): 0.2491 / 0.2748\n",
            "[10/10][170/469] Loss_D: 0.7450 Loss_G: 1.5445 D(x): 0.6882 D(G(z)): 0.2510 / 0.2555\n",
            "[10/10][180/469] Loss_D: 0.6489 Loss_G: 1.9124 D(x): 0.8290 D(G(z)): 0.3126 / 0.1842\n",
            "[10/10][190/469] Loss_D: 0.8009 Loss_G: 1.9111 D(x): 0.7259 D(G(z)): 0.3334 / 0.1887\n",
            "[10/10][200/469] Loss_D: 0.5460 Loss_G: 1.8339 D(x): 0.7912 D(G(z)): 0.2477 / 0.1863\n",
            "[10/10][200/469] Saving samples!\n",
            "[10/10][210/469] Loss_D: 0.6149 Loss_G: 1.8168 D(x): 0.8028 D(G(z)): 0.2953 / 0.1890\n",
            "[10/10][220/469] Loss_D: 0.5655 Loss_G: 2.1623 D(x): 0.8121 D(G(z)): 0.2630 / 0.1403\n",
            "[10/10][230/469] Loss_D: 0.6180 Loss_G: 1.8178 D(x): 0.6953 D(G(z)): 0.1786 / 0.2350\n",
            "[10/10][240/469] Loss_D: 0.6557 Loss_G: 1.9973 D(x): 0.7621 D(G(z)): 0.2748 / 0.1777\n",
            "[10/10][250/469] Loss_D: 0.7819 Loss_G: 2.2488 D(x): 0.7752 D(G(z)): 0.3401 / 0.1444\n",
            "[10/10][260/469] Loss_D: 0.6324 Loss_G: 1.9120 D(x): 0.8072 D(G(z)): 0.2980 / 0.1705\n",
            "[10/10][270/469] Loss_D: 0.5744 Loss_G: 1.9281 D(x): 0.7601 D(G(z)): 0.2293 / 0.1732\n",
            "[10/10][280/469] Loss_D: 0.6641 Loss_G: 1.7117 D(x): 0.7187 D(G(z)): 0.2239 / 0.2203\n",
            "[10/10][290/469] Loss_D: 0.7460 Loss_G: 1.5519 D(x): 0.6169 D(G(z)): 0.1790 / 0.2467\n",
            "[10/10][300/469] Loss_D: 0.4747 Loss_G: 1.8820 D(x): 0.7709 D(G(z)): 0.1664 / 0.1787\n",
            "[10/10][300/469] Saving samples!\n",
            "[10/10][310/469] Loss_D: 0.6858 Loss_G: 1.3875 D(x): 0.6729 D(G(z)): 0.2101 / 0.2801\n",
            "[10/10][320/469] Loss_D: 0.6058 Loss_G: 1.6002 D(x): 0.6940 D(G(z)): 0.1660 / 0.2525\n",
            "[10/10][330/469] Loss_D: 0.9983 Loss_G: 1.0605 D(x): 0.5090 D(G(z)): 0.1602 / 0.4352\n",
            "[10/10][340/469] Loss_D: 0.5709 Loss_G: 1.7161 D(x): 0.6841 D(G(z)): 0.1344 / 0.2382\n",
            "[10/10][350/469] Loss_D: 0.8265 Loss_G: 1.3078 D(x): 0.5398 D(G(z)): 0.0971 / 0.3157\n",
            "[10/10][360/469] Loss_D: 0.6435 Loss_G: 2.1124 D(x): 0.7719 D(G(z)): 0.2770 / 0.1562\n",
            "[10/10][370/469] Loss_D: 0.5133 Loss_G: 2.0428 D(x): 0.7192 D(G(z)): 0.1316 / 0.1736\n",
            "[10/10][380/469] Loss_D: 0.6626 Loss_G: 1.9928 D(x): 0.8036 D(G(z)): 0.3208 / 0.1621\n",
            "[10/10][390/469] Loss_D: 0.5879 Loss_G: 1.9295 D(x): 0.6759 D(G(z)): 0.1277 / 0.1863\n",
            "[10/10][400/469] Loss_D: 0.8932 Loss_G: 1.3633 D(x): 0.7096 D(G(z)): 0.3612 / 0.2949\n",
            "[10/10][400/469] Saving samples!\n",
            "[10/10][410/469] Loss_D: 0.5979 Loss_G: 1.8493 D(x): 0.7521 D(G(z)): 0.2269 / 0.1880\n",
            "[10/10][420/469] Loss_D: 0.4802 Loss_G: 2.2758 D(x): 0.7896 D(G(z)): 0.1712 / 0.1329\n",
            "[10/10][430/469] Loss_D: 0.6193 Loss_G: 1.6766 D(x): 0.8328 D(G(z)): 0.3235 / 0.2124\n",
            "[10/10][440/469] Loss_D: 0.6455 Loss_G: 2.1585 D(x): 0.7710 D(G(z)): 0.2906 / 0.1432\n",
            "[10/10][450/469] Loss_D: 0.6846 Loss_G: 1.6027 D(x): 0.7065 D(G(z)): 0.2340 / 0.2525\n",
            "[10/10][460/469] Loss_D: 0.5916 Loss_G: 1.8594 D(x): 0.7517 D(G(z)): 0.2190 / 0.1878\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vtl3iEHFeSzp"
      },
      "cell_type": "code",
      "source": [
        "# Save images from training data\n",
        "import imageio\n",
        "import glob\n",
        "\n",
        "img_path = '/content/results/'\n",
        "img_tmp = 'samples_fake_*.png'\n",
        "\n",
        "images = [\n",
        "    imageio.imread(f) for f in sorted(glob.glob(f\"{img_path}/{img_tmp}\"))\n",
        "]\n",
        "\n",
        "imageio.mimsave(f\"{img_path}/training.gif\", images, duration=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uGtBcGAkgeE9"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}